# Performance and Optimization in Node.js

**Difficulty:** Advanced
**Estimated Time:** 90-120 minutes
**Prerequisites:** Node.js fundamentals, async programming, database integration, profiling
**Target:** Node.js 18+ LTS, Production environments

---

## Learning Objectives

By the end of this lesson, you will be able to:

1. Profile Node.js applications to identify performance bottlenecks
2. Understand and prevent memory leaks in Node.js
3. Implement effective caching strategies (in-memory, Redis)
4. Optimize database queries for better performance
5. Configure load balancing for distributed systems
6. Choose between horizontal and vertical scaling strategies
7. Leverage CDNs for static asset delivery
8. Use APM tools for monitoring and observability
9. Optimize event loop and async operations
10. Apply performance best practices in production

---

## Table of Contents

1. [Introduction](#introduction)
2. [Profiling Node.js Applications](#profiling-nodejs-applications)
3. [Memory Management and Leaks](#memory-management-and-leaks)
4. [Caching Strategies](#caching-strategies)
5. [Database Query Optimization](#database-query-optimization)
6. [Load Balancing](#load-balancing)
7. [Scaling Strategies](#scaling-strategies)
8. [CDN Usage](#cdn-usage)
9. [Monitoring and APM Tools](#monitoring-and-apm-tools)
10. [Worked Examples](#worked-examples)
11. [Exercises](#exercises)
12. [Best Practices](#best-practices)
13. [Common Pitfalls](#common-pitfalls)
14. [Summary & Next Steps](#summary--next-steps)
15. [References](#references)

---

## Introduction

Performance optimization is critical for scalable, responsive applications. Understanding how to identify bottlenecks, optimize resource usage, and scale effectively determines application success.

**Why Performance Matters:**

- **User Experience:** Faster apps = happier users
- **Cost Efficiency:** Better resource utilization
- **Scalability:** Handle more users with same resources
- **SEO:** Google ranks faster sites higher
- **Revenue:** Speed directly impacts conversion rates

**Performance Goals:**

- Response time < 200ms for API endpoints
- Page load < 3 seconds
- Time to First Byte (TTFB) < 600ms
- Memory usage < 512MB per process
- CPU usage < 70% average

---

## Profiling Node.js Applications

### Built-in Node.js Profiler

```javascript
/**
 * Profile CPU usage with --prof flag
 *
 * node --prof app.js
 * node --prof-process isolate-0xnnnnnnnnnnnn-v8.log > processed.txt
 */

// Example: Find slow functions
const crypto = require('crypto');

function slowFunction() {
  const startTime = Date.now();

  // Simulate CPU-intensive work
  for (let i = 0; i < 1000000; i++) {
    crypto.createHash('sha256').update('test').digest('hex');
  }

  console.log(`Execution time: ${Date.now() - startTime}ms`);
}

slowFunction();

/**
 * Profile output shows:
 * - Percentage of time in each function
 * - Call stack
 * - Optimized vs unoptimized functions
 */
```

### Chrome DevTools Profiling

```javascript
/**
 * Use Chrome DevTools for profiling
 *
 * node --inspect app.js
 * Open chrome://inspect in Chrome browser
 */

import express from 'express';

const app = express();

app.get('/heavy', (req, res) => {
  // Add performance mark
  performance.mark('heavy-start');

  const result = heavyComputation();

  performance.mark('heavy-end');
  performance.measure('heavy-computation', 'heavy-start', 'heavy-end');

  const measure = performance.getEntriesByName('heavy-computation')[0];
  console.log(`Heavy computation took ${measure.duration}ms`);

  res.json({ result, duration: measure.duration });
});

function heavyComputation() {
  let result = 0;
  for (let i = 0; i < 10000000; i++) {
    result += Math.sqrt(i);
  }
  return result;
}

app.listen(3000);
```

### Performance Measurement

```javascript
/**
 * Measure performance of specific operations
 */

import { performance } from 'perf_hooks';

class PerformanceMonitor {
  constructor() {
    this.measurements = new Map();
  }

  start(label) {
    performance.mark(`${label}-start`);
  }

  end(label) {
    performance.mark(`${label}-end`);
    performance.measure(label, `${label}-start`, `${label}-end`);

    const measure = performance.getEntriesByName(label)[0];
    this.measurements.set(label, measure.duration);

    return measure.duration;
  }

  getResults() {
    return Object.fromEntries(this.measurements);
  }

  report() {
    console.log('\n=== Performance Report ===');
    for (const [label, duration] of this.measurements) {
      console.log(`${label}: ${duration.toFixed(2)}ms`);
    }
  }
}

// Usage
const monitor = new PerformanceMonitor();

async function processData() {
  monitor.start('total');

  monitor.start('fetch-data');
  const data = await fetchDataFromDB();
  monitor.end('fetch-data');

  monitor.start('transform');
  const transformed = transformData(data);
  monitor.end('transform');

  monitor.start('save');
  await saveToCache(transformed);
  monitor.end('save');

  monitor.end('total');
  monitor.report();

  return transformed;
}
```

---

## Memory Management and Leaks

### Understanding Memory in Node.js

```javascript
/**
 * Node.js memory structure
 */

const memoryUsage = process.memoryUsage();
console.log({
  rss: `${Math.round(memoryUsage.rss / 1024 / 1024)}MB`, // Resident Set Size
  heapTotal: `${Math.round(memoryUsage.heapTotal / 1024 / 1024)}MB`,
  heapUsed: `${Math.round(memoryUsage.heapUsed / 1024 / 1024)}MB`,
  external: `${Math.round(memoryUsage.external / 1024 / 1024)}MB`,
  arrayBuffers: `${Math.round(memoryUsage.arrayBuffers / 1024 / 1024)}MB`
});

/**
 * Monitor memory over time
 */

function logMemoryUsage() {
  const usage = process.memoryUsage();
  console.log({
    timestamp: new Date().toISOString(),
    heapUsed: `${Math.round(usage.heapUsed / 1024 / 1024)}MB`,
    heapTotal: `${Math.round(usage.heapTotal / 1024 / 1024)}MB`
  });
}

setInterval(logMemoryUsage, 10000); // Every 10 seconds
```

### Common Memory Leaks

```javascript
/**
 * Common memory leak patterns
 */

// ❌ LEAK 1: Global variables accumulating
let globalCache = [];

function addToCache(item) {
  globalCache.push(item); // Never cleaned up!
}

// ✅ FIX: Use LRU cache with size limit
import LRU from 'lru-cache';

const cache = new LRU({
  max: 500,
  ttl: 1000 * 60 * 5 // 5 minutes
});

function addToCache(key, item) {
  cache.set(key, item);
}

// ❌ LEAK 2: Event listeners not removed
class DataProcessor {
  constructor(eventEmitter) {
    eventEmitter.on('data', this.handleData.bind(this));
    // Listener never removed!
  }

  handleData(data) {
    console.log(data);
  }
}

// ✅ FIX: Remove listeners
class DataProcessor {
  constructor(eventEmitter) {
    this.eventEmitter = eventEmitter;
    this.handleData = this.handleData.bind(this);
    eventEmitter.on('data', this.handleData);
  }

  destroy() {
    this.eventEmitter.removeListener('data', this.handleData);
  }

  handleData(data) {
    console.log(data);
  }
}

// ❌ LEAK 3: Closures holding references
function createHandler() {
  const largeData = new Array(1000000).fill('data');

  return function() {
    console.log(largeData[0]); // Keeps entire array in memory
  };
}

// ✅ FIX: Only capture what's needed
function createHandler() {
  const largeData = new Array(1000000).fill('data');
  const firstItem = largeData[0];

  return function() {
    console.log(firstItem);
  };
}

// ❌ LEAK 4: Timers not cleared
function startPolling() {
  setInterval(() => {
    fetchData();
  }, 1000);
  // Never cleared!
}

// ✅ FIX: Clear timers
function startPolling() {
  const intervalId = setInterval(() => {
    fetchData();
  }, 1000);

  return () => clearInterval(intervalId);
}
```

### Detecting Memory Leaks

```javascript
/**
 * Heap snapshot analysis
 */

// Take heap snapshot
const v8 = require('v8');
const fs = require('fs');

function takeHeapSnapshot(filename) {
  const snapshotStream = v8.writeHeapSnapshot(filename);
  console.log(`Heap snapshot written to ${snapshotStream}`);
}

// Compare snapshots to find leaks
takeHeapSnapshot('./snapshot-1.heapsnapshot');
// ... run your app for a while
takeHeapSnapshot('./snapshot-2.heapsnapshot');

// Load in Chrome DevTools Memory profiler to compare

/**
 * Automated leak detection
 */

class MemoryLeakDetector {
  constructor(thresholdMB = 100) {
    this.thresholdMB = thresholdMB;
    this.baseline = null;
  }

  setBaseline() {
    this.baseline = process.memoryUsage().heapUsed;
  }

  check() {
    if (!this.baseline) {
      this.setBaseline();
      return;
    }

    const current = process.memoryUsage().heapUsed;
    const increaseMB = (current - this.baseline) / 1024 / 1024;

    if (increaseMB > this.thresholdMB) {
      console.warn(`⚠️  Memory increased by ${increaseMB.toFixed(2)}MB`);
      takeHeapSnapshot(`./leak-${Date.now()}.heapsnapshot`);
    }
  }
}

const detector = new MemoryLeakDetector(50);
detector.setBaseline();

setInterval(() => {
  detector.check();
}, 60000); // Check every minute
```

---

## Caching Strategies

### In-Memory Caching

```javascript
/**
 * Simple in-memory cache
 */

class SimpleCache {
  constructor(ttl = 60000) {
    this.cache = new Map();
    this.ttl = ttl;
  }

  set(key, value) {
    this.cache.set(key, {
      value,
      expires: Date.now() + this.ttl
    });
  }

  get(key) {
    const item = this.cache.get(key);

    if (!item) return null;

    if (Date.now() > item.expires) {
      this.cache.delete(key);
      return null;
    }

    return item.value;
  }

  delete(key) {
    this.cache.delete(key);
  }

  clear() {
    this.cache.clear();
  }

  size() {
    return this.cache.size;
  }
}

// Usage
const cache = new SimpleCache(300000); // 5 minutes

async function getUser(userId) {
  const cacheKey = `user:${userId}`;

  // Check cache
  const cached = cache.get(cacheKey);
  if (cached) {
    console.log('Cache hit');
    return cached;
  }

  // Fetch from database
  console.log('Cache miss');
  const user = await User.findById(userId);

  // Store in cache
  cache.set(cacheKey, user);

  return user;
}
```

### Redis Caching

```javascript
/**
 * Redis caching for distributed systems
 *
 * npm install ioredis
 */

import Redis from 'ioredis';

const redis = new Redis({
  host: process.env.REDIS_HOST || 'localhost',
  port: process.env.REDIS_PORT || 6379,
  password: process.env.REDIS_PASSWORD,
  maxRetriesPerRequest: 3,
  retryStrategy(times) {
    const delay = Math.min(times * 50, 2000);
    return delay;
  }
});

class RedisCache {
  constructor(redis, defaultTTL = 3600) {
    this.redis = redis;
    this.defaultTTL = defaultTTL;
  }

  async get(key) {
    const value = await this.redis.get(key);
    return value ? JSON.parse(value) : null;
  }

  async set(key, value, ttl = this.defaultTTL) {
    await this.redis.setex(key, ttl, JSON.stringify(value));
  }

  async delete(key) {
    await this.redis.del(key);
  }

  async deletePattern(pattern) {
    const keys = await this.redis.keys(pattern);
    if (keys.length > 0) {
      await this.redis.del(...keys);
    }
  }

  async exists(key) {
    return (await this.redis.exists(key)) === 1;
  }
}

const cache = new RedisCache(redis);

// Usage
async function getUserWithCache(userId) {
  const cacheKey = `user:${userId}`;

  // Try cache
  const cached = await cache.get(cacheKey);
  if (cached) return cached;

  // Fetch from DB
  const user = await User.findById(userId);

  // Cache for 1 hour
  await cache.set(cacheKey, user, 3600);

  return user;
}

// Invalidate cache on update
async function updateUser(userId, updates) {
  const user = await User.findByIdAndUpdate(userId, updates, { new: true });

  // Invalidate cache
  await cache.delete(`user:${userId}`);

  return user;
}
```

### Cache Patterns

```javascript
/**
 * Advanced caching patterns
 */

// 1. Cache-Aside (Lazy Loading)
async function cacheAside(key, fetchFn, ttl = 3600) {
  const cached = await cache.get(key);
  if (cached) return cached;

  const data = await fetchFn();
  await cache.set(key, data, ttl);
  return data;
}

// 2. Write-Through Cache
async function writeThrough(key, data, saveFn, ttl = 3600) {
  // Write to both cache and database
  await Promise.all([
    cache.set(key, data, ttl),
    saveFn(data)
  ]);
}

// 3. Write-Behind Cache (Write-Back)
class WriteBehindCache {
  constructor(cache, flushInterval = 5000) {
    this.cache = cache;
    this.pendingWrites = new Map();

    setInterval(() => this.flush(), flushInterval);
  }

  async set(key, value, saveFn) {
    // Update cache immediately
    await this.cache.set(key, value);

    // Queue database write
    this.pendingWrites.set(key, { value, saveFn });
  }

  async flush() {
    for (const [key, { value, saveFn }] of this.pendingWrites) {
      try {
        await saveFn(value);
        this.pendingWrites.delete(key);
      } catch (error) {
        console.error(`Failed to write ${key}:`, error);
      }
    }
  }
}

// 4. Refresh-Ahead Cache
class RefreshAheadCache {
  constructor(cache, threshold = 0.8) {
    this.cache = cache;
    this.threshold = threshold;
  }

  async get(key, fetchFn, ttl = 3600) {
    const cached = await this.cache.get(key);

    if (cached) {
      // Check if approaching expiration
      const remaining = await this.cache.ttl(key);
      if (remaining < ttl * this.threshold) {
        // Refresh in background
        this.refreshInBackground(key, fetchFn, ttl);
      }
      return cached;
    }

    // Cache miss
    const data = await fetchFn();
    await this.cache.set(key, data, ttl);
    return data;
  }

  async refreshInBackground(key, fetchFn, ttl) {
    try {
      const data = await fetchFn();
      await this.cache.set(key, data, ttl);
    } catch (error) {
      console.error(`Background refresh failed for ${key}:`, error);
    }
  }
}
```

---

## Database Query Optimization

### Indexing Strategies

```javascript
/**
 * MongoDB indexes
 */

import mongoose from 'mongoose';

const userSchema = new mongoose.Schema({
  email: { type: String, unique: true },
  username: String,
  firstName: String,
  lastName: String,
  createdAt: { type: Date, default: Date.now }
});

// Single field index
userSchema.index({ email: 1 });

// Compound index (for queries using both fields)
userSchema.index({ lastName: 1, firstName: 1 });

// Text index for full-text search
userSchema.index({ username: 'text', firstName: 'text', lastName: 'text' });

// Sparse index (only for documents with field)
userSchema.index({ profileImage: 1 }, { sparse: true });

// TTL index (auto-delete after time)
userSchema.index({ createdAt: 1 }, { expireAfterSeconds: 2592000 }); // 30 days

/**
 * PostgreSQL indexes
 */

const createIndexes = async () => {
  // B-tree index (default)
  await pool.query('CREATE INDEX idx_users_email ON users(email)');

  // Composite index
  await pool.query('CREATE INDEX idx_users_name ON users(last_name, first_name)');

  // Partial index
  await pool.query('CREATE INDEX idx_active_users ON users(email) WHERE is_active = true');

  // GIN index for JSONB
  await pool.query('CREATE INDEX idx_users_metadata ON users USING GIN(metadata)');

  // Full-text search index
  await pool.query(`
    CREATE INDEX idx_posts_search
    ON posts
    USING GIN(to_tsvector('english', title || ' ' || content))
  `);
};
```

### Query Optimization

```javascript
/**
 * Optimize MongoDB queries
 */

// ❌ BAD: N+1 query problem
async function getUsersWithPosts() {
  const users = await User.find();

  for (const user of users) {
    user.posts = await Post.find({ authorId: user._id }); // N queries!
  }

  return users;
}

// ✅ GOOD: Use aggregation
async function getUsersWithPosts() {
  return await User.aggregate([
    {
      $lookup: {
        from: 'posts',
        localField: '_id',
        foreignField: 'authorId',
        as: 'posts'
      }
    }
  ]);
}

// ❌ BAD: Loading unnecessary fields
async function getUsers() {
  return await User.find(); // Loads all fields including password!
}

// ✅ GOOD: Select only needed fields
async function getUsers() {
  return await User.find().select('username email createdAt -_id');
}

// ❌ BAD: No pagination
async function getPosts() {
  return await Post.find(); // Could be millions!
}

// ✅ GOOD: Paginate results
async function getPosts(page = 1, limit = 20) {
  const skip = (page - 1) * limit;

  const [posts, total] = await Promise.all([
    Post.find()
      .sort({ createdAt: -1 })
      .skip(skip)
      .limit(limit),
    Post.countDocuments()
  ]);

  return {
    posts,
    pagination: {
      page,
      limit,
      total,
      pages: Math.ceil(total / limit)
    }
  };
}

/**
 * Optimize PostgreSQL queries
 */

// Use EXPLAIN ANALYZE to see query plan
const analyzeQuery = async () => {
  const result = await pool.query(`
    EXPLAIN ANALYZE
    SELECT u.*, COUNT(p.id) as post_count
    FROM users u
    LEFT JOIN posts p ON u.id = p.author_id
    GROUP BY u.id
    ORDER BY post_count DESC
    LIMIT 10
  `);

  console.log(result.rows);
};

// ❌ BAD: Multiple round trips
async function getPostsWithAuthors() {
  const posts = await pool.query('SELECT * FROM posts');

  for (const post of posts.rows) {
    const author = await pool.query('SELECT * FROM users WHERE id = $1', [post.author_id]);
    post.author = author.rows[0];
  }

  return posts.rows;
}

// ✅ GOOD: Single JOIN query
async function getPostsWithAuthors() {
  const result = await pool.query(`
    SELECT
      p.*,
      json_build_object(
        'id', u.id,
        'username', u.username,
        'email', u.email
      ) as author
    FROM posts p
    JOIN users u ON p.author_id = u.id
  `);

  return result.rows;
}
```

### Query Caching

```javascript
/**
 * Cache expensive queries
 */

async function getPopularPosts(limit = 10) {
  const cacheKey = `popular-posts:${limit}`;

  // Check cache
  const cached = await cache.get(cacheKey);
  if (cached) return cached;

  // Expensive query
  const posts = await Post.aggregate([
    {
      $lookup: {
        from: 'comments',
        localField: '_id',
        foreignField: 'postId',
        as: 'comments'
      }
    },
    {
      $addFields: {
        commentCount: { $size: '$comments' },
        popularity: {
          $add: ['$views', { $multiply: [{ $size: '$comments' }, 10] }]
        }
      }
    },
    { $sort: { popularity: -1 } },
    { $limit: limit }
  ]);

  // Cache for 5 minutes
  await cache.set(cacheKey, posts, 300);

  return posts;
}
```

---

## Load Balancing

### Nginx Load Balancer

```nginx
# nginx.conf

upstream backend {
    # Load balancing methods:

    # 1. Round Robin (default)
    server backend1.example.com:3000;
    server backend2.example.com:3000;
    server backend3.example.com:3000;

    # 2. Least Connections
    # least_conn;

    # 3. IP Hash (sticky sessions)
    # ip_hash;

    # 4. Weighted Round Robin
    # server backend1.example.com:3000 weight=3;
    # server backend2.example.com:3000 weight=2;
    # server backend3.example.com:3000 weight=1;

    # Health checks
    server backend1.example.com:3000 max_fails=3 fail_timeout=30s;
    server backend2.example.com:3000 max_fails=3 fail_timeout=30s;

    # Backup server
    server backup.example.com:3000 backup;

    # Connection pool
    keepalive 32;
}

server {
    listen 80;
    server_name api.example.com;

    location / {
        proxy_pass http://backend;
        proxy_http_version 1.1;

        # Headers
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        # Timeouts
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;

        # Connection reuse
        proxy_set_header Connection "";

        # Buffering
        proxy_buffering on;
        proxy_buffer_size 4k;
        proxy_buffers 8 4k;
    }
}
```

### Node.js Cluster Mode

```javascript
/**
 * Cluster mode for multi-core utilization
 */

import cluster from 'cluster';
import os from 'os';
import process from 'process';

const numCPUs = os.cpus().length;

if (cluster.isPrimary) {
  console.log(`Primary ${process.pid} is running`);

  // Fork workers
  for (let i = 0; i < numCPUs; i++) {
    cluster.fork();
  }

  cluster.on('exit', (worker, code, signal) => {
    console.log(`Worker ${worker.process.pid} died`);
    // Replace dead worker
    cluster.fork();
  });

  // Graceful shutdown
  process.on('SIGTERM', () => {
    console.log('SIGTERM received, shutting down gracefully');

    for (const id in cluster.workers) {
      cluster.workers[id].kill();
    }
  });

} else {
  // Workers share TCP connection
  import('./app.js').then(({ default: app }) => {
    const PORT = process.env.PORT || 3000;

    app.listen(PORT, () => {
      console.log(`Worker ${process.pid} started on port ${PORT}`);
    });
  });
}

/**
 * Alternative: PM2 cluster mode
 *
 * pm2 start app.js -i max  # Start workers for all CPUs
 * pm2 start app.js -i 4    # Start 4 workers
 */
```

---

## Scaling Strategies

### Horizontal vs Vertical Scaling

```javascript
/**
 * Scaling comparison
 */

const scalingStrategies = {
  vertical: {
    description: 'Add more power to existing server',
    pros: [
      'Simple - no code changes',
      'No distributed system complexity',
      'Better for single-threaded workloads'
    ],
    cons: [
      'Hardware limits (max CPU/RAM)',
      'Single point of failure',
      'Expensive at scale',
      'Downtime for upgrades'
    ],
    when: 'Small to medium applications, limited traffic'
  },

  horizontal: {
    description: 'Add more servers',
    pros: [
      'No hardware limits',
      'High availability',
      'Cost-effective at scale',
      'Rolling deployments (zero downtime)'
    ],
    cons: [
      'Complex - need load balancer',
      'Distributed system challenges',
      'Session management',
      'Data consistency issues'
    ],
    when: 'High traffic, need redundancy, microservices'
  }
};
```

### Auto-Scaling Configuration

```javascript
/**
 * AWS Auto Scaling example (pseudocode)
 */

const autoScalingConfig = {
  minInstances: 2,
  maxInstances: 10,
  desiredCapacity: 3,

  scaleUpPolicy: {
    metric: 'CPUUtilization',
    threshold: 70, // percent
    evaluationPeriods: 2,
    period: 60, // seconds
    action: 'AddInstances',
    scalingAdjustment: 2
  },

  scaleDownPolicy: {
    metric: 'CPUUtilization',
    threshold: 30,
    evaluationPeriods: 5,
    period: 60,
    action: 'RemoveInstances',
    scalingAdjustment: -1
  },

  cooldown: 300 // seconds between scaling actions
};

/**
 * Kubernetes Horizontal Pod Autoscaler
 */

const k8sHPAConfig = `
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: backend-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: backend
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
`;
```

---

## CDN Usage

### CDN Configuration

```javascript
/**
 * Serve static assets via CDN
 */

// Environment-based CDN URL
const CDN_URL = process.env.NODE_ENV === 'production'
  ? 'https://cdn.example.com'
  : '';

// Helper function
function assetUrl(path) {
  return `${CDN_URL}${path}`;
}

// In templates
const imageUrl = assetUrl('/images/logo.png');
// Development: /images/logo.png
// Production: https://cdn.example.com/images/logo.png

/**
 * CloudFront configuration example
 */

const cloudFrontConfig = {
  origins: [
    {
      domainName: 's3-bucket.s3.amazonaws.com',
      originPath: '/static',
      customHeaders: {
        'X-Origin-Key': 'secret-key'
      }
    }
  ],

  cacheBehaviors: {
    images: {
      pathPattern: '/images/*',
      ttl: {
        min: 86400, // 1 day
        default: 2592000, // 30 days
        max: 31536000 // 1 year
      },
      compress: true
    },
    api: {
      pathPattern: '/api/*',
      ttl: {
        min: 0,
        default: 0,
        max: 0
      },
      allowedMethods: ['GET', 'HEAD', 'OPTIONS', 'PUT', 'POST', 'PATCH', 'DELETE']
    }
  },

  priceClass: 'PriceClass_100', // Use only North America and Europe
  geoRestriction: {
    restrictionType: 'whitelist',
    locations: ['US', 'CA', 'GB', 'DE']
  }
};
```

### Cache Control Headers

```javascript
/**
 * Set appropriate cache headers
 */

import express from 'express';

const app = express();

// Static assets - long cache
app.use('/static', express.static('public', {
  maxAge: '1y', // 1 year
  immutable: true
}));

// Custom cache headers
app.get('/api/config', (req, res) => {
  res.set({
    'Cache-Control': 'public, max-age=300', // 5 minutes
    'ETag': generateEtag(config)
  });
  res.json(config);
});

// No cache for dynamic content
app.get('/api/user/profile', (req, res) => {
  res.set({
    'Cache-Control': 'private, no-cache, no-store, must-revalidate',
    'Pragma': 'no-cache',
    'Expires': '0'
  });
  res.json(userProfile);
});

/**
 * Cache busting with versioned URLs
 */

const VERSION = Date.now();

function versionedUrl(path) {
  return `${path}?v=${VERSION}`;
}

// <script src="/js/app.js?v=1234567890"></script>
```

---

## Monitoring and APM Tools

### Application Performance Monitoring

```javascript
/**
 * New Relic integration
 */

// newrelic.js
export default {
  app_name: ['My Application'],
  license_key: process.env.NEW_RELIC_LICENSE_KEY,
  logging: {
    level: 'info'
  },
  allow_all_headers: true,
  attributes: {
    exclude: [
      'request.headers.cookie',
      'request.headers.authorization'
    ]
  }
};

// app.js
import newrelic from 'newrelic';

// Custom metrics
newrelic.recordMetric('Custom/Orders/Processed', ordersCount);

// Custom transactions
app.get('/api/heavy-operation', async (req, res) => {
  const transaction = newrelic.getTransaction();

  // Add custom attributes
  transaction.addCustomAttribute('userId', req.user.id);
  transaction.addCustomAttribute('complexity', 'high');

  const result = await heavyOperation();
  res.json(result);
});

/**
 * Prometheus metrics
 */

import promClient from 'prom-client';

// Create registry
const register = new promClient.Registry();

// Default metrics
promClient.collectDefaultMetrics({ register });

// Custom metrics
const httpRequestDuration = new promClient.Histogram({
  name: 'http_request_duration_seconds',
  help: 'Duration of HTTP requests in seconds',
  labelNames: ['method', 'route', 'status_code'],
  buckets: [0.1, 0.5, 1, 2, 5]
});

register.registerMetric(httpRequestDuration);

// Middleware
app.use((req, res, next) => {
  const start = Date.now();

  res.on('finish', () => {
    const duration = (Date.now() - start) / 1000;

    httpRequestDuration
      .labels(req.method, req.route?.path || req.path, res.statusCode)
      .observe(duration);
  });

  next();
});

// Metrics endpoint
app.get('/metrics', async (req, res) => {
  res.set('Content-Type', register.contentType);
  res.end(await register.metrics());
});
```

### Health Checks

```javascript
/**
 * Comprehensive health checks
 */

import express from 'express';
import mongoose from 'mongoose';
import redis from './redis.js';

const app = express();

app.get('/health', async (req, res) => {
  const health = {
    uptime: process.uptime(),
    timestamp: Date.now(),
    status: 'ok',
    checks: {}
  };

  // Database check
  try {
    await mongoose.connection.db.admin().ping();
    health.checks.database = { status: 'up' };
  } catch (error) {
    health.checks.database = { status: 'down', error: error.message };
    health.status = 'degraded';
  }

  // Redis check
  try {
    await redis.ping();
    health.checks.redis = { status: 'up' };
  } catch (error) {
    health.checks.redis = { status: 'down', error: error.message };
    health.status = 'degraded';
  }

  // Memory check
  const memUsage = process.memoryUsage();
  const memUsedMB = memUsage.heapUsed / 1024 / 1024;
  const memTotalMB = memUsage.heapTotal / 1024 / 1024;
  const memPercent = (memUsedMB / memTotalMB) * 100;

  health.checks.memory = {
    status: memPercent > 90 ? 'warning' : 'ok',
    used: `${memUsedMB.toFixed(2)}MB`,
    total: `${memTotalMB.toFixed(2)}MB`,
    percentage: `${memPercent.toFixed(2)}%`
  };

  // CPU check
  const cpuUsage = process.cpuUsage();
  health.checks.cpu = {
    user: cpuUsage.user,
    system: cpuUsage.system
  };

  const statusCode = health.status === 'ok' ? 200 : 503;
  res.status(statusCode).json(health);
});

// Readiness check (for Kubernetes)
app.get('/ready', async (req, res) => {
  try {
    await mongoose.connection.db.admin().ping();
    res.status(200).json({ status: 'ready' });
  } catch (error) {
    res.status(503).json({ status: 'not ready' });
  }
});

// Liveness check
app.get('/live', (req, res) => {
  res.status(200).json({ status: 'alive' });
});
```

---

## Worked Examples

### Example 1: Performance Optimization

```javascript
/**
 * Optimize slow endpoint
 */

// Before: Slow endpoint
app.get('/api/dashboard', async (req, res) => {
  // Multiple sequential DB queries
  const user = await User.findById(req.user.id);
  const posts = await Post.find({ authorId: user.id });
  const comments = await Comment.find({ authorId: user.id });
  const stats = await getStats(user.id);

  res.json({ user, posts, comments, stats });
});

// Execution time: ~800ms

// After: Optimized
app.get('/api/dashboard', async (req, res) => {
  const cacheKey = `dashboard:${req.user.id}`;

  // Check cache
  const cached = await cache.get(cacheKey);
  if (cached) {
    return res.json(cached);
  }

  // Parallel queries
  const [user, posts, comments, stats] = await Promise.all([
    User.findById(req.user.id).select('username email avatar'),
    Post.find({ authorId: req.user.id }).select('title createdAt').limit(10),
    Comment.countDocuments({ authorId: req.user.id }),
    getStatsCached(req.user.id)
  ]);

  const data = { user, posts, commentCount: comments, stats };

  // Cache for 5 minutes
  await cache.set(cacheKey, data, 300);

  res.json(data);
});

// Execution time: ~150ms (cache miss), ~2ms (cache hit)
```

### Example 2: Memory Leak Fix

```javascript
/**
 * Fix memory leak in event processing
 */

// Before: Memory leak
class EventProcessor {
  constructor() {
    this.listeners = [];

    // Creates new listener for each instance
    setInterval(() => {
      this.processEvents();
    }, 1000);
  }

  on(event, callback) {
    this.listeners.push({ event, callback });
  }

  processEvents() {
    // Process events
  }
}

// Every new instance creates timer that's never cleared!

// After: Fixed
class EventProcessor {
  constructor() {
    this.listeners = [];
    this.timerId = null;
    this.isRunning = false;
  }

  start() {
    if (this.isRunning) return;

    this.isRunning = true;
    this.timerId = setInterval(() => {
      this.processEvents();
    }, 1000);
  }

  stop() {
    if (this.timerId) {
      clearInterval(this.timerId);
      this.timerId = null;
    }
    this.isRunning = false;
  }

  destroy() {
    this.stop();
    this.listeners = [];
  }

  on(event, callback) {
    this.listeners.push({ event, callback });
  }

  off(event, callback) {
    this.listeners = this.listeners.filter(
      l => l.event !== event || l.callback !== callback
    );
  }

  processEvents() {
    // Process events
  }
}

// Usage
const processor = new EventProcessor();
processor.start();

// Clean up when done
process.on('SIGTERM', () => {
  processor.destroy();
});
```

---

## Exercises

### Exercise 1: Implement Caching (Easy)

**Problem:** Add caching to reduce database load.

```javascript
/**
 * Add Redis caching to these functions
 */

async function getProducts() {
  return await Product.find();
}

async function getProductById(id) {
  return await Product.findById(id);
}

// Implement caching with 1 hour TTL
```

**Solution:** See `solutions/exercise1_performance.js`

---

### Exercise 2: Optimize Query (Medium)

**Problem:** Fix N+1 query problem.

```javascript
/**
 * Optimize this code
 */

async function getBlogPostsWithAuthors() {
  const posts = await Post.find().limit(50);

  for (const post of posts) {
    post.author = await User.findById(post.authorId);
  }

  return posts;
}

// Reduce to single query
```

**Solution:** See `solutions/exercise2_performance.js`

---

### Exercise 3: Memory Leak Detection (Medium)

**Problem:** Find and fix memory leak.

```javascript
/**
 * This code has a memory leak. Find and fix it.
 */

class WebSocketManager {
  constructor() {
    this.connections = new Map();
  }

  addConnection(socket) {
    const id = generateId();
    this.connections.set(id, socket);

    socket.on('message', (data) => {
      this.handleMessage(id, data);
    });
  }

  handleMessage(id, data) {
    console.log(`Message from ${id}:`, data);
  }
}

// What's leaking? How to fix?
```

**Solution:** See `solutions/exercise3_performance.js`

---

### Exercise 4: Load Balancing Setup (Hard)

**Problem:** Configure Nginx load balancer.

```nginx
# Set up load balancing with:
# - 3 backend servers
# - Health checks
# - SSL termination
# - Static file caching
```

**Solution:** See `solutions/exercise4_performance.nginx`

---

### Exercise 5: APM Integration (Hard)

**Problem:** Add comprehensive monitoring.

```javascript
/**
 * Integrate:
 * - Request duration tracking
 * - Error rate monitoring
 * - Custom business metrics
 * - Health checks
 */
```

**Solution:** See `solutions/exercise5_performance.js`

---

## Best Practices

1. **Profile before optimizing** - Measure, don't guess
2. **Cache aggressively** - But invalidate properly
3. **Use indexes** - For all queried fields
4. **Paginate results** - Never return unbounded data
5. **Monitor memory** - Detect leaks early
6. **Load balance** - Distribute traffic evenly
7. **Use CDNs** - For static assets
8. **Implement health checks** - For reliability
9. **Set up monitoring** - Know your metrics
10. **Optimize queries** - N+1 is the enemy

---

## Common Pitfalls

1. **Premature optimization** - Optimize after profiling
2. **Ignoring indexes** - Slow queries
3. **No caching strategy** - Repeated expensive operations
4. **Memory leaks** - Not cleaning up listeners/timers
5. **Blocking event loop** - CPU-intensive synchronous code
6. **No monitoring** - Flying blind
7. **Single point of failure** - No redundancy
8. **Inefficient queries** - Loading unnecessary data
9. **No load testing** - Surprises in production
10. **Ignoring CDN** - Slow static asset delivery

---

## Summary & Next Steps

### Key Takeaways

- Profile first, optimize second
- Caching is powerful but needs proper invalidation
- Database indexes are critical for performance
- Memory leaks accumulate over time - monitor closely
- Load balancing enables horizontal scaling
- CDNs dramatically improve static asset delivery
- APM tools provide visibility into production performance

### Related Topics

- [Database Integration](./36.Database_Integration.md)
- [Security Best Practices](./41.Security_Best_Practices.md)
- [Deployment and DevOps](./42.Deployment_And_DevOps.md)

### Further Reading

- [Node.js Performance Best Practices](https://nodejs.org/en/docs/guides/simple-profiling/)
- [Web Performance](https://web.dev/performance/)
- [Database Indexing Guide](https://use-the-index-luke.com/)

---

## References

- Node.js Profiling: https://nodejs.org/en/docs/guides/simple-profiling/
- Redis Documentation: https://redis.io/documentation
- Nginx Load Balancing: https://docs.nginx.com/nginx/admin-guide/load-balancer/
- New Relic: https://docs.newrelic.com/
- Prometheus: https://prometheus.io/docs/

---

**Next Lesson:** [Security Best Practices](./41.Security_Best_Practices.md)
