# Streams and Buffers

**Difficulty:** Advanced
**Estimated Time:** 95-110 minutes
**Prerequisites:** File system operations, async/await, events, memory management
**Target:** Node.js 18+ LTS

---

## Learning Objectives

By the end of this lesson, you will be able to:

1. Understand streams and their types (Readable, Writable, Duplex, Transform)
2. Work with Buffers for binary data
3. Read large files efficiently with streams
4. Pipe streams together
5. Handle stream events and errors
6. Implement backpressure handling
7. Create custom Transform streams
8. Build real-world streaming applications
9. Optimize memory usage with streams
10. Debug stream-related issues

---

## Table of Contents

1. [Introduction](#introduction)
2. [What Are Streams](#what-are-streams)
3. [Buffers Explained](#buffers-explained)
4. [Readable Streams](#readable-streams)
5. [Writable Streams](#writable-streams)
6. [Duplex Streams](#duplex-streams)
7. [Transform Streams](#transform-streams)
8. [Piping Streams](#piping-streams)
9. [Stream Events](#stream-events)
10. [Backpressure](#backpressure)
11. [Real-World Streaming](#real-world-streaming)
12. [Worked Examples](#worked-examples)
13. [Exercises](#exercises)
14. [Testing & Verification](#testing--verification)
15. [Best Practices](#best-practices)
16. [Common Pitfalls](#common-pitfalls)
17. [Performance Considerations](#performance-considerations)
18. [Summary & Next Steps](#summary--next-steps)
19. [References](#references)

---

## Introduction

**Streams** are collections of data that might not be available all at once. They allow you to process data piece by piece without loading everything into memory.

**Why Streams Matter:**

- **Memory Efficiency:** Process large files without loading all into RAM
- **Time Efficiency:** Start processing before all data arrives
- **Composability:** Chain operations together
- **Real-time Processing:** Handle data as it arrives
- **Backpressure:** Automatic flow control

**Real-World Applications:**

- **File Processing:** Read/write large files
- **HTTP Requests:** Stream request/response bodies
- **Video Streaming:** Netflix, YouTube
- **Data Pipelines:** ETL processes
- **Log Processing:** Parse massive log files

---

## What Are Streams

### Stream Types

```javascript
/**
 * Four types of streams in Node.js
 */

const STREAM_TYPES = {
  Readable: {
    description: 'Source of data (you can read from it)',
    examples: [
      'fs.createReadStream()',
      'http.IncomingMessage (request)',
      'process.stdin'
    ],
    methods: ['read()', 'pipe()', 'unpipe()'],
    events: ['data', 'end', 'error', 'close']
  },

  Writable: {
    description: 'Destination for data (you can write to it)',
    examples: [
      'fs.createWriteStream()',
      'http.ServerResponse (response)',
      'process.stdout'
    ],
    methods: ['write()', 'end()'],
    events: ['drain', 'finish', 'error', 'close']
  },

  Duplex: {
    description: 'Both readable and writable',
    examples: [
      'net.Socket',
      'tls.TLSSocket',
      'TCP sockets'
    ],
    methods: ['Both Readable and Writable methods'],
    events: ['All Readable and Writable events']
  },

  Transform: {
    description: 'Duplex stream that modifies data',
    examples: [
      'zlib.createGzip()',
      'crypto.createCipher()',
      'Custom transforms'
    ],
    methods: ['Both Readable and Writable methods'],
    events: ['All Duplex events']
  }
};

/**
 * Stream flow visualization
 *
 * Readable → Transform → Transform → Writable
 *  (source)   (modify)    (modify)    (destination)
 *
 * Example:
 * File → Compress → Encrypt → Network
 */
```

### Basic Stream Example

```javascript
/**
 * Simple stream example
 */

const fs = require('fs');

// Without streams (loads entire file into memory)
fs.readFile('large-file.txt', (err, data) => {
  console.log('File size:', data.length);
  // Problem: 1GB file = 1GB RAM usage!
});

// With streams (chunks loaded incrementally)
const readStream = fs.createReadStream('large-file.txt');

readStream.on('data', (chunk) => {
  console.log('Received chunk:', chunk.length, 'bytes');
  // Each chunk is ~64KB by default
});

readStream.on('end', () => {
  console.log('Finished reading file');
  // Total memory used: ~64KB at a time!
});
```

---

## Buffers Explained

### Understanding Buffers

```javascript
/**
 * Buffers - Fixed-size chunks of binary data
 */

// Create buffers
const buf1 = Buffer.alloc(10);           // Allocate 10 bytes (filled with 0)
const buf2 = Buffer.from('Hello');       // From string
const buf3 = Buffer.from([1, 2, 3, 4]);  // From array
const buf4 = Buffer.allocUnsafe(10);     // Faster but uninitialized

// Buffer methods
console.log(buf2.toString());            // 'Hello'
console.log(buf2.toString('hex'));       // '48656c6c6f'
console.log(buf2.toString('base64'));    // 'SGVsbG8='
console.log(buf2.length);                // 5

// Write to buffer
const buf = Buffer.alloc(10);
buf.write('Hi');
console.log(buf.toString());             // 'Hi\x00\x00\x00\x00\x00\x00\x00\x00'

// Read from buffer
const buf5 = Buffer.from('Hello World');
console.log(buf5[0]);                    // 72 (ASCII 'H')
console.log(buf5.slice(0, 5).toString()); // 'Hello'

// Concatenate buffers
const buf6 = Buffer.from('Hello ');
const buf7 = Buffer.from('World');
const combined = Buffer.concat([buf6, buf7]);
console.log(combined.toString());        // 'Hello World'

// Compare buffers
const bufA = Buffer.from('ABC');
const bufB = Buffer.from('ABC');
const bufC = Buffer.from('ABD');
console.log(bufA.equals(bufB));          // true
console.log(bufA.equals(bufC));          // false
console.log(Buffer.compare(bufA, bufC)); // -1 (bufA < bufC)

// Buffer is like Uint8Array
console.log(buf2 instanceof Uint8Array); // true
```

### Binary Data Operations

```javascript
/**
 * Working with binary data
 */

// Reading binary file
const fs = require('fs');

fs.readFile('image.png', (err, buffer) => {
  console.log('Image size:', buffer.length, 'bytes');
  
  // First few bytes (PNG signature)
  console.log('PNG signature:', buffer.slice(0, 8));
  // <Buffer 89 50 4e 47 0d 0a 1a 0a>
});

// Writing binary data
const imageBuffer = Buffer.from(/* binary data */);
fs.writeFile('output.png', imageBuffer, (err) => {
  console.log('Binary file written');
});

// Encoding/Decoding
const data = 'Secret message';
const encoded = Buffer.from(data).toString('base64');
const decoded = Buffer.from(encoded, 'base64').toString('utf8');

console.log('Encoded:', encoded);        // U2VjcmV0IG1lc3NhZ2U=
console.log('Decoded:', decoded);        // Secret message
```

---

## Readable Streams

### Creating Readable Streams

```javascript
/**
 * Working with Readable streams
 */

const fs = require('fs');
const { Readable } = require('stream');

// 1. File read stream
const fileStream = fs.createReadStream('large-file.txt', {
  encoding: 'utf8',
  highWaterMark: 64 * 1024  // 64KB chunks (default)
});

fileStream.on('data', (chunk) => {
  console.log('Chunk size:', chunk.length);
});

fileStream.on('end', () => {
  console.log('Reading complete');
});

// 2. Custom readable stream
class NumberStream extends Readable {
  constructor(max) {
    super();
    this.current = 0;
    this.max = max;
  }

  _read() {
    if (this.current <= this.max) {
      this.push(this.current.toString() + '\n');
      this.current++;
    } else {
      this.push(null);  // Signal end of stream
    }
  }
}

const numbers = new NumberStream(10);
numbers.on('data', (chunk) => {
  console.log('Number:', chunk.toString().trim());
});

// 3. Stream from array
function* generateData() {
  yield 'First chunk';
  yield 'Second chunk';
  yield 'Third chunk';
}

const arrayStream = Readable.from(generateData());
arrayStream.on('data', (chunk) => {
  console.log('Data:', chunk);
});

// 4. Async iteration
async function processStream() {
  const stream = fs.createReadStream('file.txt', 'utf8');
  
  for await (const chunk of stream) {
    console.log('Processing chunk:', chunk.length);
    // Process chunk
  }
  
  console.log('Stream processing complete');
}
```

### Reading Modes

```javascript
/**
 * Readable stream modes: flowing vs paused
 */

const fs = require('fs');

// Flowing mode (automatic)
const flowingStream = fs.createReadStream('file.txt');

flowingStream.on('data', (chunk) => {
  console.log('Auto chunk:', chunk.length);
});

// Paused mode (manual)
const pausedStream = fs.createReadStream('file.txt');

pausedStream.on('readable', () => {
  let chunk;
  
  while ((chunk = pausedStream.read()) !== null) {
    console.log('Manual chunk:', chunk.length);
  }
});

// Switching modes
const stream = fs.createReadStream('file.txt');

// Start paused
stream.pause();

setTimeout(() => {
  console.log('Resuming stream...');
  stream.resume();  // Switch to flowing
}, 1000);

stream.on('data', (chunk) => {
  console.log('Chunk:', chunk.length);
});
```

---

## Writable Streams

### Creating Writable Streams

```javascript
/**
 * Working with Writable streams
 */

const fs = require('fs');
const { Writable } = require('stream');

// 1. File write stream
const writeStream = fs.createWriteStream('output.txt');

writeStream.write('First line\n');
writeStream.write('Second line\n');
writeStream.end('Final line\n');  // Close stream

writeStream.on('finish', () => {
  console.log('Writing complete');
});

// 2. Custom writable stream
class Logger extends Writable {
  _write(chunk, encoding, callback) {
    const message = `[${new Date().toISOString()}] ${chunk}`;
    console.log(message);
    callback();  // Signal write complete
  }
}

const logger = new Logger();
logger.write('Log entry 1\n');
logger.write('Log entry 2\n');
logger.end();

// 3. Writing with backpressure handling
async function writeWithBackpressure(stream, data) {
  for (const item of data) {
    const canContinue = stream.write(item);
    
    if (!canContinue) {
      // Buffer is full, wait for drain
      await new Promise(resolve => stream.once('drain', resolve));
    }
  }
  
  stream.end();
}

const ws = fs.createWriteStream('large-output.txt');
const data = Array.from({ length: 1000000 }, (_, i) => `Line ${i}\n`);

writeWithBackpressure(ws, data);
```

---

## Transform Streams

### Creating Transform Streams

```javascript
/**
 * Transform streams modify data
 */

const { Transform } = require('stream');
const fs = require('fs');

// 1. Uppercase transform
class UpperCaseTransform extends Transform {
  _transform(chunk, encoding, callback) {
    this.push(chunk.toString().toUpperCase());
    callback();
  }
}

fs.createReadStream('input.txt')
  .pipe(new UpperCaseTransform())
  .pipe(fs.createWriteStream('output.txt'));

// 2. CSV to JSON transform
class CSVToJSON extends Transform {
  constructor() {
    super({ objectMode: true });
    this.headers = null;
  }

  _transform(line, encoding, callback) {
    const data = line.toString().trim().split(',');
    
    if (!this.headers) {
      this.headers = data;
    } else {
      const obj = {};
      this.headers.forEach((header, i) => {
        obj[header] = data[i];
      });
      this.push(JSON.stringify(obj) + '\n');
    }
    
    callback();
  }
}

// 3. Compression transform
const zlib = require('zlib');

fs.createReadStream('file.txt')
  .pipe(zlib.createGzip())
  .pipe(fs.createWriteStream('file.txt.gz'));

// 4. Encryption transform
const crypto = require('crypto');

const encrypt = crypto.createCipher('aes-256-cbc', 'password');

fs.createReadStream('secret.txt')
  .pipe(encrypt)
  .pipe(fs.createWriteStream('secret.enc'));
```

---

## Piping Streams

### Stream Piping

```javascript
/**
 * Piping connects streams together
 */

const fs = require('fs');
const zlib = require('zlib');

// Simple pipe
fs.createReadStream('input.txt')
  .pipe(fs.createWriteStream('output.txt'));

// Multiple pipes
fs.createReadStream('input.txt')
  .pipe(zlib.createGzip())
  .pipe(fs.createWriteStream('input.txt.gz'));

// Complex pipeline
const { pipeline } = require('stream');

pipeline(
  fs.createReadStream('input.txt'),
  zlib.createGzip(),
  fs.createWriteStream('output.txt.gz'),
  (err) => {
    if (err) {
      console.error('Pipeline failed:', err);
    } else {
      console.log('Pipeline succeeded');
    }
  }
);

// With error handling
fs.createReadStream('input.txt')
  .on('error', (err) => console.error('Read error:', err))
  .pipe(zlib.createGzip())
  .on('error', (err) => console.error('Gzip error:', err))
  .pipe(fs.createWriteStream('output.gz'))
  .on('error', (err) => console.error('Write error:', err))
  .on('finish', () => console.log('Done'));
```

---

## Stream Events

### Essential Events

```javascript
/**
 * Stream events
 */

const fs = require('fs');

const readStream = fs.createReadStream('file.txt');

// data: New chunk available
readStream.on('data', (chunk) => {
  console.log('Data:', chunk.length, 'bytes');
});

// end: No more data
readStream.on('end', () => {
  console.log('Reading finished');
});

// error: Error occurred
readStream.on('error', (err) => {
  console.error('Error:', err);
});

// close: Stream closed
readStream.on('close', () => {
  console.log('Stream closed');
});

// Writable stream events
const writeStream = fs.createWriteStream('output.txt');

// drain: Buffer emptied, can write more
writeStream.on('drain', () => {
  console.log('Buffer drained, ready for more');
});

// finish: All data written
writeStream.on('finish', () => {
  console.log('Writing finished');
});

// pipe: Stream piped to this writable
writeStream.on('pipe', (src) => {
  console.log('Piped from:', src.constructor.name);
});
```

---

## Backpressure

### Understanding Backpressure

```javascript
/**
 * Backpressure: When destination can't keep up with source
 */

const fs = require('fs');

// Without backpressure handling (BAD)
const read = fs.createReadStream('large-file.txt');
const write = fs.createWriteStream('output.txt');

read.on('data', (chunk) => {
  write.write(chunk);  // Ignores backpressure!
});

// With backpressure handling (GOOD)
const read2 = fs.createReadStream('large-file.txt');
const write2 = fs.createWriteStream('output.txt');

read2.on('data', (chunk) => {
  const canContinue = write2.write(chunk);
  
  if (!canContinue) {
    console.log('Buffer full, pausing read');
    read2.pause();
  }
});

write2.on('drain', () => {
  console.log('Buffer drained, resuming read');
  read2.resume();
});

// Using pipe (handles backpressure automatically)
fs.createReadStream('large-file.txt')
  .pipe(fs.createWriteStream('output.txt'));
```

---

## Real-World Streaming

### Example 1: HTTP File Upload

```javascript
/**
 * Streaming file upload
 */

const express = require('express');
const fs = require('fs');
const app = express();

app.post('/upload', (req, res) => {
  const writeStream = fs.createWriteStream('uploaded-file.txt');
  
  req.pipe(writeStream);
  
  writeStream.on('finish', () => {
    res.json({ success: true, message: 'File uploaded' });
  });
  
  writeStream.on('error', (err) => {
    res.status(500).json({ error: err.message });
  });
});

app.listen(3000);
```

### Example 2: Log Processing

```javascript
/**
 * Process large log file
 */

const fs = require('fs');
const readline = require('readline');

async function analyzeLogs(filePath) {
  const fileStream = fs.createReadStream(filePath);
  
  const rl = readline.createInterface({
    input: fileStream,
    crlfDelay: Infinity
  });
  
  const stats = {
    total: 0,
    errors: 0,
    warnings: 0
  };
  
  for await (const line of rl) {
    stats.total++;
    
    if (line.includes('ERROR')) stats.errors++;
    if (line.includes('WARN')) stats.warnings++;
  }
  
  return stats;
}

analyzeLogs('app.log').then(stats => {
  console.log('Log stats:', stats);
});
```

---

## Summary & Next Steps

### Key Takeaways

1. **Streams** enable memory-efficient data processing
2. **Buffers** handle binary data
3. **Four stream types:** Readable, Writable, Duplex, Transform
4. **Piping** connects streams elegantly
5. **Backpressure** prevents memory issues

### What's Next?

- **Process Management**: Environment variables, child processes
- **Cluster Module**: Multi-core utilization
- **Worker Threads**: CPU-intensive tasks

---

## References

- [Node.js Streams](https://nodejs.org/api/stream.html)
- [Stream Handbook](https://github.com/substack/stream-handbook)
- [Buffer Documentation](https://nodejs.org/api/buffer.html)

---

**Next Lesson:** [35. Process and Child Processes](./35.Process_And_Child_Processes.md)

**Previous Lesson:** [33. RESTful API Design](./33.RESTful_API_Design.md)
