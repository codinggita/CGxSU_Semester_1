# Conditional Probability and Independence

## Learning Objectives

- Understand conditional probability and its applications
- Master the multiplication rule and chain rule
- Define and verify statistical independence
- Apply Bayes' theorem for probability updating
- Solve problems involving dependent and independent events

## Prerequisites

- Probability foundations (sample spaces, events, axioms)
- Set theory and operations
- Basic probability calculations

## Definitions and Theoretical Foundations

### Definition 2.1 (Conditional Probability)

The **conditional probability** of event $A$ given event $B$ has occurred is:

$$P(A|B) = \frac{P(A \cap B)}{P(B)}, \quad P(B) > 0$$

**Interpretation:** The probability of $A$ in the reduced sample space where $B$ is known to have occurred.

### Theorem 2.1 (Multiplication Rule)

For any two events $A$ and $B$ with $P(B) > 0$:

$$P(A \cap B) = P(B) \cdot P(A|B)$$

**Proof:** Rearranging the definition of conditional probability yields the result immediately. $\square$

### Theorem 2.2 (Chain Rule)

For events $A_1, A_2, \ldots, A_n$ with positive probabilities:

$$P(A_1 \cap A_2 \cap \cdots \cap A_n) = P(A_1) \cdot P(A_2|A_1) \cdot P(A_3|A_1 \cap A_2) \cdots P(A_n|A_1 \cap \cdots \cap A_{n-1})$$

### Definition 2.2 (Independence)

Events $A$ and $B$ are **independent** if:

$$P(A \cap B) = P(A) \cdot P(B)$$

Equivalently (when $P(B) > 0$):

$$P(A|B) = P(A)$$

**Interpretation:** Knowledge of $B$ does not change the probability of $A$.

### Theorem 2.3 (Properties of Independence)

If $A$ and $B$ are independent, then:
1. $A$ and $B^c$ are independent
2. $A^c$ and $B$ are independent
3. $A^c$ and $B^c$ are independent

**Proof of (1):**

$$P(A \cap B^c) = P(A) - P(A \cap B) = P(A) - P(A)P(B) = P(A)(1 - P(B)) = P(A)P(B^c)$$ $\square$

### Definition 2.3 (Mutual Independence)

Events $A_1, A_2, \ldots, A_n$ are **mutually independent** if for every subset $\{i_1, \ldots, i_k\}$:

$$P(A_{i_1} \cap \cdots \cap A_{i_k}) = P(A_{i_1}) \cdots P(A_{i_k})$$

## Worked Examples

### Example 1 (Simple): Conditional Probability with Cards

**Problem:** A card is drawn from a standard deck. Find the probability it's a King given that it's a face card.

**Solution:**

Let $K$ = King, $F$ = Face card

- $P(K \cap F) = \frac{4}{52}$ (4 Kings are face cards)
- $P(F) = \frac{12}{52}$ (J, Q, K in 4 suits)

$$P(K|F) = \frac{P(K \cap F)}{P(F)} = \frac{4/52}{12/52} = \frac{4}{12} = \frac{1}{3}$$

**Answer:** $\frac{1}{3}$ $\square$

---

### Example 2 (Intermediate): Multiplication Rule

**Problem:** A box contains 3 red and 2 blue balls. Two balls are drawn without replacement. Find $P(\text{both red})$.

**Solution:**

Let $R_1$ = first red, $R_2$ = second red

$$P(R_1 \cap R_2) = P(R_1) \cdot P(R_2|R_1)$$

- $P(R_1) = \frac{3}{5}$
- $P(R_2|R_1) = \frac{2}{4} = \frac{1}{2}$ (2 red left out of 4 total)

$$P(R_1 \cap R_2) = \frac{3}{5} \cdot \frac{1}{2} = \frac{3}{10}$$

**Answer:** $\frac{3}{10}$ $\square$

---

### Example 3 (Intermediate): Independence Verification

**Problem:** Two dice are rolled. Let $A$ = "first die shows 6" and $B$ = "sum is 7". Are $A$ and $B$ independent?

**Solution:**

Sample space: $|S| = 36$

- $P(A) = \frac{6}{36} = \frac{1}{6}$ (6 outcomes: (6,1), (6,2), ..., (6,6))
- $P(B) = \frac{6}{36} = \frac{1}{6}$ (6 outcomes: (1,6), (2,5), (3,4), (4,3), (5,2), (6,1))
- $P(A \cap B) = \frac{1}{36}$ (only (6,1))

Check: $P(A) \cdot P(B) = \frac{1}{6} \cdot \frac{1}{6} = \frac{1}{36} = P(A \cap B)$ ✓

**Answer:** Yes, $A$ and $B$ are independent. $\square$

---

### Example 4 (Challenging): Law of Total Probability

**Problem:** A factory has 3 machines: $M_1, M_2, M_3$ producing 30%, 45%, 25% of items respectively. Defect rates are 2%, 3%, 4%. Find the probability a randomly selected item is defective.

**Solution:**

Let $D$ = defective. By Law of Total Probability:

$$P(D) = P(M_1)P(D|M_1) + P(M_2)P(D|M_2) + P(M_3)P(D|M_3)$$
$$= 0.30(0.02) + 0.45(0.03) + 0.25(0.04)$$
$$= 0.006 + 0.0135 + 0.01 = 0.0295$$

**Answer:** $P(D) = 0.0295$ or $2.95\%$ $\square$

---

### Example 5 (Challenging): Bayes' Theorem

**Problem:** Continuing Example 4, if an item is defective, what's the probability it came from $M_2$?

**Solution:**

By Bayes' Theorem:

$$P(M_2|D) = \frac{P(M_2)P(D|M_2)}{P(D)} = \frac{0.45 \times 0.03}{0.0295} = \frac{0.0135}{0.0295} \approx 0.458$$

**Answer:** $P(M_2|D) \approx 45.8\%$ $\square$

## Exercises

### Exercise 1

Two cards are drawn without replacement. Find $P(\text{both aces})$.

**Solution:** $\frac{4}{52} \cdot \frac{3}{51} = \frac{1}{221}$ ✓

---

### Exercise 2

If $P(A) = 0.6$, $P(B) = 0.4$, and $A, B$ are independent, find $P(A \cap B)$ and $P(A \cup B)$.

**Solution:**
- $P(A \cap B) = 0.6 \times 0.4 = 0.24$
- $P(A \cup B) = 0.6 + 0.4 - 0.24 = 0.76$ ✓

---

### Exercise 3

Prove that if $A$ and $B$ are independent and disjoint, then $P(A) = 0$ or $P(B) = 0$.

**Solution:** If $A \cap B = \emptyset$ and independent:
$0 = P(A \cap B) = P(A)P(B)$

Therefore $P(A) = 0$ or $P(B) = 0$. $\square$

## Applications

- **Medicine:** Disease diagnosis given test results
- **Machine Learning:** Naive Bayes classifiers
- **Finance:** Risk assessment and portfolio analysis
- **Engineering:** Reliability and fault tree analysis

## Common Mistakes

1. **Confusing $P(A|B)$ with $P(B|A)$:** These are generally different
2. **Assuming independence without verification:** Must check $P(A \cap B) = P(A)P(B)$
3. **Misusing multiplication rule:** Only valid when using conditional probabilities correctly

## Summary

1. **Conditional probability:** $P(A|B) = \frac{P(A \cap B)}{P(B)}$
2. **Multiplication rule:** $P(A \cap B) = P(A)P(B|A) = P(B)P(A|B)$
3. **Independence:** $P(A \cap B) = P(A)P(B)$ iff $P(A|B) = P(A)$
4. **Law of Total Probability:** Partition sample space for complex calculations
5. **Bayes' Theorem:** Update probabilities given new evidence

---

**Further Study:** Bayes' theorem applications, random variables, conditional expectation.
