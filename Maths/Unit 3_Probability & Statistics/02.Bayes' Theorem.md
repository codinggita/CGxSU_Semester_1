# Bayes' Theorem and Law of Total Probability

## Learning Objectives

- Understand and apply the Law of Total Probability
- Master Bayes' Theorem and its applications
- Solve probability updating problems
- Apply Bayesian reasoning to diagnostic and classification problems
- Understand prior, likelihood, and posterior probabilities

## Prerequisites

- Conditional probability and independence
- Multiplication rule
- Basic set theory and partitions

## Definitions and Theoretical Foundations

### Definition 3.1 (Partition)

A collection of events $\{B_1, B_2, \ldots, B_n\}$ forms a **partition** of the sample space $S$ if:
1. $B_i \cap B_j = \emptyset$ for all $i \neq j$ (mutually exclusive)
2. $\bigcup_{i=1}^{n} B_i = S$ (exhaustive)
3. $P(B_i) > 0$ for all $i$

**Interpretation:** The events partition $S$ into disjoint pieces that cover the entire space.

### Theorem 3.1 (Law of Total Probability)

Let $\{B_1, B_2, \ldots, B_n\}$ be a partition of $S$. For any event $A$:

$$P(A) = \sum_{i=1}^{n} P(B_i) P(A|B_i)$$

**Proof:**

Since $\{B_1, \ldots, B_n\}$ partition $S$, we can write:
$$A = A \cap S = A \cap \left(\bigcup_{i=1}^{n} B_i\right) = \bigcup_{i=1}^{n} (A \cap B_i)$$

The events $\{A \cap B_i\}$ are mutually exclusive, so by Axiom 3:
$$P(A) = \sum_{i=1}^{n} P(A \cap B_i) = \sum_{i=1}^{n} P(B_i) P(A|B_i)$$ $\square$

### Theorem 3.2 (Bayes' Theorem)

Let $\{B_1, B_2, \ldots, B_n\}$ be a partition of $S$ with $P(B_i) > 0$. For any event $A$ with $P(A) > 0$:

$$P(B_j|A) = \frac{P(B_j) P(A|B_j)}{\sum_{i=1}^{n} P(B_i) P(A|B_i)}$$

**Proof:**

By the definition of conditional probability:
$$P(B_j|A) = \frac{P(B_j \cap A)}{P(A)} = \frac{P(B_j) P(A|B_j)}{P(A)}$$

Applying the Law of Total Probability to the denominator yields the result. $\square$

**Bayesian Terminology:**
- $P(B_j)$: **Prior probability** (before observing $A$)
- $P(A|B_j)$: **Likelihood** (probability of evidence given hypothesis)
- $P(B_j|A)$: **Posterior probability** (after observing $A$)

### Theorem 3.3 (Bayes' Theorem - Two Events)

For events $A$ and $B$ with $P(A), P(B) > 0$:

$$P(B|A) = \frac{P(A|B) P(B)}{P(A|B) P(B) + P(A|B^c) P(B^c)}$$

**Interpretation:** This form is useful when we have a binary hypothesis (e.g., disease/no disease).

## Worked Examples

### Example 1 (Simple): Law of Total Probability

**Problem:** A factory has two machines: $M_1$ produces 60% of items, $M_2$ produces 40%. Find the probability of selecting an item from the factory.

**Solution:**

Let $A$ = "item selected", $M_1$ = "from machine 1", $M_2$ = "from machine 2"

Given: $P(M_1) = 0.6$, $P(M_2) = 0.4$, and $P(A|M_1) = P(A|M_2) = 1$ (all items can be selected)

By Law of Total Probability:
$$P(A) = P(M_1)P(A|M_1) + P(M_2)P(A|M_2) = 0.6(1) + 0.4(1) = 1$$ ✓

**Answer:** $P(A) = 1$ (as expected) $\square$

---

### Example 2 (Intermediate): Medical Diagnosis

**Problem:** A disease affects 1% of the population. A test is 95% accurate for infected people and 90% accurate for healthy people. If someone tests positive, what's the probability they have the disease?

**Solution:**

Let $D$ = disease, $T$ = positive test

**Given:**
- $P(D) = 0.01$, $P(D^c) = 0.99$
- $P(T|D) = 0.95$ (sensitivity)
- $P(T^c|D^c) = 0.90$, so $P(T|D^c) = 0.10$ (false positive rate)

**Find:** $P(D|T)$

**Step 1:** Find $P(T)$ using Law of Total Probability:
$$P(T) = P(D)P(T|D) + P(D^c)P(T|D^c)$$
$$= 0.01(0.95) + 0.99(0.10) = 0.0095 + 0.099 = 0.1085$$

**Step 2:** Apply Bayes' Theorem:
$$P(D|T) = \frac{P(D)P(T|D)}{P(T)} = \frac{0.01 \times 0.95}{0.1085} = \frac{0.0095}{0.1085} \approx 0.0876$$

**Answer:** Only about 8.76% of people who test positive actually have the disease! $\square$

**Key Insight:** Even with a highly accurate test, the low base rate (1%) means most positive tests are false positives.

---

### Example 3 (Intermediate): Three Machines

**Problem:** A factory has three machines with the following characteristics:

| Machine | Production % | Defect Rate |
|---------|-------------|-------------|
| $M_1$   | 30%         | 1%          |
| $M_2$   | 50%         | 2%          |
| $M_3$   | 20%         | 3%          |

(a) Find the probability a randomly selected item is defective.
(b) If an item is defective, find the probability it came from $M_2$.

**Solution:**

Let $D$ = defective item

**(a)** By Law of Total Probability:
$$P(D) = P(M_1)P(D|M_1) + P(M_2)P(D|M_2) + P(M_3)P(D|M_3)$$
$$= 0.30(0.01) + 0.50(0.02) + 0.20(0.03)$$
$$= 0.003 + 0.010 + 0.006 = 0.019$$

**(b)** By Bayes' Theorem:
$$P(M_2|D) = \frac{P(M_2)P(D|M_2)}{P(D)} = \frac{0.50 \times 0.02}{0.019} = \frac{0.010}{0.019} \approx 0.526$$

**Answers:**
(a) $P(D) = 1.9\%$
(b) $P(M_2|D) \approx 52.6\%$ $\square$

---

### Example 4 (Challenging): Email Classification

**Problem:** An email filter classifies messages as spam or legitimate. Historical data shows:
- 40% of emails are spam
- 80% of spam emails contain the word "free"
- 10% of legitimate emails contain "free"

An email contains "free". What's the probability it's spam?

**Solution:**

Let $S$ = spam, $F$ = contains "free"

**Given:**
- $P(S) = 0.40$, $P(S^c) = 0.60$
- $P(F|S) = 0.80$
- $P(F|S^c) = 0.10$

**Find:** $P(S|F)$

**Step 1:** Find $P(F)$:
$$P(F) = P(S)P(F|S) + P(S^c)P(F|S^c)$$
$$= 0.40(0.80) + 0.60(0.10) = 0.32 + 0.06 = 0.38$$

**Step 2:** Apply Bayes' Theorem:
$$P(S|F) = \frac{P(S)P(F|S)}{P(F)} = \frac{0.40 \times 0.80}{0.38} = \frac{0.32}{0.38} \approx 0.842$$

**Answer:** About 84.2% probability the email is spam. $\square$

---

### Example 5 (Challenging): Sequential Testing

**Problem:** A student takes a two-stage exam. The probability of passing stage 1 is 0.7. Given passing stage 1, the probability of passing stage 2 is 0.8. Given failing stage 1, the probability of passing stage 2 is 0.3. What is the overall probability of passing stage 2?

**Solution:**

Let $P_1$ = pass stage 1, $P_2$ = pass stage 2

**Given:**
- $P(P_1) = 0.7$, so $P(P_1^c) = 0.3$
- $P(P_2|P_1) = 0.8$
- $P(P_2|P_1^c) = 0.3$

By Law of Total Probability:
$$P(P_2) = P(P_1)P(P_2|P_1) + P(P_1^c)P(P_2|P_1^c)$$
$$= 0.7(0.8) + 0.3(0.3) = 0.56 + 0.09 = 0.65$$

**Answer:** $P(P_2) = 0.65$ or 65% $\square$

---

### Example 6 (Advanced): Bayesian Updating

**Problem:** A bag contains either 3 red and 2 blue balls (hypothesis $H_1$) or 2 red and 3 blue balls (hypothesis $H_2$). Initially, we believe each hypothesis is equally likely. We draw a ball and observe it's red. Update the probability of $H_1$.

**Solution:**

**Prior:** $P(H_1) = P(H_2) = 0.5$

Let $R$ = red ball drawn

**Likelihoods:**
- $P(R|H_1) = \frac{3}{5}$ (3 red out of 5)
- $P(R|H_2) = \frac{2}{5}$ (2 red out of 5)

**Step 1:** Find $P(R)$:
$$P(R) = P(H_1)P(R|H_1) + P(H_2)P(R|H_2)$$
$$= 0.5 \times \frac{3}{5} + 0.5 \times \frac{2}{5} = \frac{3}{10} + \frac{2}{10} = \frac{5}{10} = 0.5$$

**Step 2:** Posterior for $H_1$:
$$P(H_1|R) = \frac{P(H_1)P(R|H_1)}{P(R)} = \frac{0.5 \times \frac{3}{5}}{0.5} = \frac{3/10}{1/2} = \frac{3}{5} = 0.6$$

**Answer:** After observing red, $P(H_1|R) = 0.6$ (updated from 0.5) $\square$

## Exercises

### Exercise 1

A company has two suppliers: A (70% of parts) and B (30% of parts). Defect rates are 2% for A and 5% for B. Find the probability a randomly selected part is defective.

**Hint:** Use Law of Total Probability with the partition $\{A, B\}$.

**Solution:**
$$P(D) = 0.70(0.02) + 0.30(0.05) = 0.014 + 0.015 = 0.029$$ ✓

---

### Exercise 2

Continuing Exercise 1, if a part is defective, find the probability it came from supplier B.

**Hint:** Use Bayes' Theorem with $P(D)$ from Exercise 1.

**Solution:**
$$P(B|D) = \frac{0.30 \times 0.05}{0.029} = \frac{0.015}{0.029} \approx 0.517$$ ✓

---

### Exercise 3

A student studies for an exam with probability 0.6. If they study, the probability of passing is 0.9. If they don't study, the probability is 0.3. Find the overall probability of passing.

**Hint:** Let $S$ = study, $P$ = pass. Find $P(P)$ using total probability.

**Solution:**
$$P(P) = 0.6(0.9) + 0.4(0.3) = 0.54 + 0.12 = 0.66$$ ✓

---

### Exercise 4

In a certain city, 60% of residents own cars. Among car owners, 30% subscribe to a traffic app. Among non-car owners, 10% subscribe. If a randomly selected resident subscribes to the app, what's the probability they own a car?

**Hint:** Apply Bayes' Theorem with the partition {car owner, non-car owner}.

**Solution:** Let $C$ = car owner, $A$ = app subscriber.
$$P(A) = 0.6(0.3) + 0.4(0.1) = 0.18 + 0.04 = 0.22$$
$$P(C|A) = \frac{0.6 \times 0.3}{0.22} = \frac{0.18}{0.22} \approx 0.818$$ ✓

---

### Exercise 5

Prove that if $A$ and $B$ are independent, then $P(B|A) = P(B)$, showing that Bayes' theorem reduces to the prior probability.

**Hint:** Use the definition of independence: $P(A \cap B) = P(A)P(B)$.

**Solution:** If independent:
$$P(B|A) = \frac{P(A \cap B)}{P(A)} = \frac{P(A)P(B)}{P(A)} = P(B)$$
The posterior equals the prior (no update from observing $A$). ✓

## Applications

### Medical Diagnosis
Bayes' theorem is fundamental in interpreting diagnostic test results, accounting for disease prevalence and test accuracy.

### Spam Filtering
Email filters use Bayesian classification to update spam probabilities based on word frequencies.

### Machine Learning
Naive Bayes classifiers are based on Bayes' theorem and widely used in text classification and pattern recognition.

### Risk Assessment
Insurance and finance use Bayesian updating to refine risk estimates as new data becomes available.

### Scientific Inference
Bayesian statistics provides a framework for updating scientific hypotheses based on experimental evidence.

## Common Mistakes

1. **Ignoring base rates:** Failing to account for prior probabilities leads to incorrect posterior probabilities (base rate fallacy).

2. **Confusing $P(A|B)$ and $P(B|A)$:** These are generally different! Bayes' theorem shows how they relate.

3. **Incorrect partitions:** Ensure events are mutually exclusive and exhaustive when applying the Law of Total Probability.

4. **Arithmetic errors:** Carefully calculate the denominator in Bayes' theorem; it's often the main source of errors.

## Summary

1. **Law of Total Probability:** $P(A) = \sum_{i} P(B_i)P(A|B_i)$ for a partition $\{B_i\}$

2. **Bayes' Theorem:** $P(B_j|A) = \frac{P(B_j)P(A|B_j)}{\sum_i P(B_i)P(A|B_i)}$

3. **Bayesian updating:** Combines prior knowledge $P(B)$ with new evidence $P(A|B)$ to get posterior $P(B|A)$

4. **Applications:** Essential for diagnostic reasoning, classification, and decision-making under uncertainty

5. **Key insight:** Even highly accurate tests can have low positive predictive value when the condition is rare

---

**Further Study:** Bayesian networks, prior distributions, conjugate priors, Bayesian inference.
