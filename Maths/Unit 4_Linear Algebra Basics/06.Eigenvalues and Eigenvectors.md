# Eigenvalues and Eigenvectors

## Learning Objectives

- Define eigenvalues and eigenvectors rigorously
- Compute eigenvalues using the characteristic equation
- Find eigenvectors for given eigenvalues
- Understand geometric and algebraic significance
- Apply diagonalization when possible
- Recognize eigenvalue applications in science and engineering

## Prerequisites

- Matrix operations and determinants
- Solving linear systems
- Polynomial equations
- Vector operations and linear independence

## Definitions

### Definition 7.1 (Eigenvalue and Eigenvector)

Let $A$ be an $n \times n$ square matrix. A nonzero vector $\mathbf{v} \in \mathbb{R}^n$ is an **eigenvector** of $A$ if:

$$A\mathbf{v} = \lambda \mathbf{v}$$

for some scalar $\lambda \in \mathbb{R}$ (or $\mathbb{C}$). The scalar $\lambda$ is called an **eigenvalue** of $A$ corresponding to eigenvector $\mathbf{v}$.

**Key insight:** $A$ transforms $\mathbf{v}$ by simply scaling it (no rotation), with scale factor $\lambda$.

**Important:** $\mathbf{v} \neq \mathbf{0}$ by definition, but $\lambda$ can be zero.

### Definition 7.2 (Characteristic Equation)

The **characteristic equation** of $A$ is:

$$\det(A - \lambda I) = 0$$

**Rationale:** $A\mathbf{v} = \lambda\mathbf{v} \iff (A - \lambda I)\mathbf{v} = \mathbf{0}$ has nontrivial solution $\iff \det(A - \lambda I) = 0$.

### Definition 7.3 (Characteristic Polynomial)

The **characteristic polynomial** of $A$ is:

$$p(\lambda) = \det(A - \lambda I)$$

This is a polynomial of degree $n$ in $\lambda$.

**Roots** of $p(\lambda) = 0$ are the eigenvalues.

### Definition 7.4 (Eigenspace)

For eigenvalue $\lambda$, the **eigenspace** $E_\lambda$ is the set of all eigenvectors corresponding to $\lambda$, plus the zero vector:

$$E_\lambda = \{\mathbf{v} : (A - \lambda I)\mathbf{v} = \mathbf{0}\} = \text{null}(A - \lambda I)$$

This is a subspace of $\mathbb{R}^n$.

## Computing Eigenvalues and Eigenvectors

### Algorithm 7.1 (Finding Eigenvalues and Eigenvectors)

**Step 1: Find eigenvalues**
1. Form $A - \lambda I$
2. Compute $\det(A - \lambda I)$ to get characteristic polynomial
3. Solve $\det(A - \lambda I) = 0$ for $\lambda$

**Step 2: Find eigenvectors for each $\lambda$**
1. Substitute eigenvalue into $(A - \lambda I)\mathbf{v} = \mathbf{0}$
2. Solve the homogeneous system using row reduction
3. General solution gives eigenvectors (excluding $\mathbf{0}$)

## Properties of Eigenvalues

### Theorem 7.1 (Eigenvalues of Special Matrices)

1. **Triangular matrix:** Eigenvalues are the diagonal entries
2. **Identity:** Only eigenvalue is $\lambda = 1$ (with multiplicity $n$)
3. **Zero matrix:** Only eigenvalue is $\lambda = 0$
4. **Diagonal matrix:** Eigenvalues are the diagonal entries

### Theorem 7.2 (Properties of Eigenvalues)

For $n \times n$ matrix $A$:

1. $\det(A) = \lambda_1 \lambda_2 \cdots \lambda_n$ (product of eigenvalues)
2. $\text{trace}(A) = \lambda_1 + \lambda_2 + \cdots + \lambda_n$ (sum of eigenvalues)
3. $A$ is invertible $\iff$ no eigenvalue is zero
4. Eigenvalues of $A^T$ equal eigenvalues of $A$
5. Eigenvalues of $A^{-1}$ are $1/\lambda_1, \ldots, 1/\lambda_n$
6. Eigenvalues of $A^k$ are $\lambda_1^k, \ldots, \lambda_n^k$

### Theorem 7.3 (Linear Independence of Eigenvectors)

Eigenvectors corresponding to **distinct** eigenvalues are linearly independent.

**Proof sketch:** Use induction on number of distinct eigenvalues. $\square$

## Diagonalization

### Definition 7.5 (Diagonalizable Matrix)

A matrix $A$ is **diagonalizable** if there exists an invertible matrix $P$ and diagonal matrix $D$ such that:

$$A = PDP^{-1}$$

Equivalently: $P^{-1}AP = D$ (similarity transformation).

### Theorem 7.4 (Diagonalization Theorem)

An $n \times n$ matrix $A$ is diagonalizable if and only if $A$ has $n$ linearly independent eigenvectors.

**Construction:** If $\mathbf{v}_1, \ldots, \mathbf{v}_n$ are independent eigenvectors with eigenvalues $\lambda_1, \ldots, \lambda_n$, then:

$$P = [\mathbf{v}_1\ \mathbf{v}_2\ \cdots\ \mathbf{v}_n], \quad D = \begin{bmatrix} \lambda_1 & 0 & \cdots & 0 \\ 0 & \lambda_2 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \lambda_n \end{bmatrix}$$

**Verification:** $AP = PD$ column by column: $A\mathbf{v}_i = \lambda_i \mathbf{v}_i$.

### Theorem 7.5 (Sufficient Condition for Diagonalizability)

If an $n \times n$ matrix has $n$ **distinct** eigenvalues, then it is diagonalizable.

**Proof:** Distinct eigenvalues give $n$ independent eigenvectors (Theorem 7.3). $\square$

**Note:** Converse is false: can be diagonalizable without $n$ distinct eigenvalues (e.g., identity matrix).

## Worked Examples

### Example 1 (Simple): 2×2 Eigenvalues

**Problem:** Find eigenvalues of $A = \begin{bmatrix} 4 & 1 \\ 2 & 3 \end{bmatrix}$.

**Solution:**

$$A - \lambda I = \begin{bmatrix} 4-\lambda & 1 \\ 2 & 3-\lambda \end{bmatrix}$$

$$\det(A - \lambda I) = (4-\lambda)(3-\lambda) - 2 = \lambda^2 - 7\lambda + 10$$

$$\lambda^2 - 7\lambda + 10 = 0 \Rightarrow (\lambda - 5)(\lambda - 2) = 0$$

**Answer:** $\lambda_1 = 5$, $\lambda_2 = 2$ $\square$

---

### Example 2 (Intermediate): Finding Eigenvectors

**Problem:** Find eigenvectors for $\lambda = 5$ and $\lambda = 2$ from Example 1.

**Solution:**

**For $\lambda = 5$:**

$$(A - 5I)\mathbf{v} = \begin{bmatrix} -1 & 1 \\ 2 & -2 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$$

Row reduce: $-v_1 + v_2 = 0 \Rightarrow v_2 = v_1$. Let $v_1 = 1$:

$$\mathbf{v}_1 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}$$

**For $\lambda = 2$:**

$$(A - 2I)\mathbf{v} = \begin{bmatrix} 2 & 1 \\ 2 & 1 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$$

Row reduce: $2v_1 + v_2 = 0 \Rightarrow v_2 = -2v_1$. Let $v_1 = 1$:

$$\mathbf{v}_2 = \begin{bmatrix} 1 \\ -2 \end{bmatrix}$$

**Answer:** Eigenvectors are $\begin{bmatrix} 1 \\ 1 \end{bmatrix}$ (for $\lambda=5$) and $\begin{bmatrix} 1 \\ -2 \end{bmatrix}$ (for $\lambda=2$) $\square$

---

### Example 3 (Intermediate): Triangular Matrix

**Problem:** Find eigenvalues of $A = \begin{bmatrix} 2 & 5 & 3 \\ 0 & -1 & 7 \\ 0 & 0 & 4 \end{bmatrix}$ (upper triangular).

**Solution:**

For triangular matrix, eigenvalues are diagonal entries.

**Answer:** $\lambda_1 = 2$, $\lambda_2 = -1$, $\lambda_3 = 4$ $\square$

---

### Example 4 (Challenging): Diagonalization

**Problem:** Diagonalize $A = \begin{bmatrix} 4 & 1 \\ 2 & 3 \end{bmatrix}$ using eigenvalues and eigenvectors from Examples 1-2.

**Solution:**

From Examples 1-2: eigenvalues $\lambda_1 = 5, \lambda_2 = 2$ with eigenvectors $\mathbf{v}_1 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}, \mathbf{v}_2 = \begin{bmatrix} 1 \\ -2 \end{bmatrix}$

$$P = \begin{bmatrix} 1 & 1 \\ 1 & -2 \end{bmatrix}, \quad D = \begin{bmatrix} 5 & 0 \\ 0 & 2 \end{bmatrix}$$

Compute $P^{-1}$:
$$\det(P) = -2 - 1 = -3$$
$$P^{-1} = \frac{1}{-3}\begin{bmatrix} -2 & -1 \\ -1 & 1 \end{bmatrix} = \begin{bmatrix} 2/3 & 1/3 \\ 1/3 & -1/3 \end{bmatrix}$$

**Verification:** $A = PDP^{-1}$ (can verify by multiplication).

**Answer:** $A = \begin{bmatrix} 1 & 1 \\ 1 & -2 \end{bmatrix} \begin{bmatrix} 5 & 0 \\ 0 & 2 \end{bmatrix} \begin{bmatrix} 2/3 & 1/3 \\ 1/3 & -1/3 \end{bmatrix}$ $\square$

---

### Example 5 (Challenging): Power of Matrix

**Problem:** Using diagonalization from Example 4, compute $A^{10}$.

**Solution:**

Since $A = PDP^{-1}$:

$$A^{10} = (PDP^{-1})^{10} = PD^{10}P^{-1}$$

$$D^{10} = \begin{bmatrix} 5^{10} & 0 \\ 0 & 2^{10} \end{bmatrix} = \begin{bmatrix} 9765625 & 0 \\ 0 & 1024 \end{bmatrix}$$

$$A^{10} = \begin{bmatrix} 1 & 1 \\ 1 & -2 \end{bmatrix} \begin{bmatrix} 9765625 & 0 \\ 0 & 1024 \end{bmatrix} \begin{bmatrix} 2/3 & 1/3 \\ 1/3 & -1/3 \end{bmatrix}$$

(Computation yields specific result.)

**Key insight:** Diagonalization makes computing high powers efficient. $\square$

---

### Example 6 (Application): Population Dynamics

**Problem:** A population model has transition matrix $A = \begin{bmatrix} 0.8 & 0.3 \\ 0.2 & 0.7 \end{bmatrix}$. Find the steady-state distribution (eigenvector for $\lambda = 1$).

**Solution:**

Solve $(A - I)\mathbf{v} = \mathbf{0}$:

$$\begin{bmatrix} -0.2 & 0.3 \\ 0.2 & -0.3 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$$

From first row: $-0.2v_1 + 0.3v_2 = 0 \Rightarrow v_1 = 1.5v_2$

Let $v_2 = 2$: $\mathbf{v} = \begin{bmatrix} 3 \\ 2 \end{bmatrix}$

Normalize: $\begin{bmatrix} 3/5 \\ 2/5 \end{bmatrix} = \begin{bmatrix} 0.6 \\ 0.4 \end{bmatrix}$

**Answer:** Steady state: 60% in state 1, 40% in state 2. $\square$

## Exercises

### Exercise 1

Find eigenvalues of $A = \begin{bmatrix} 3 & 0 \\ 0 & 5 \end{bmatrix}$ (diagonal matrix).

**Hint:** Eigenvalues are diagonal entries.

**Solution:** $\lambda_1 = 3$, $\lambda_2 = 5$ 

---

### Exercise 2

Find characteristic polynomial of $A = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}$.

**Hint:** Compute $\det(A - \lambda I)$.

**Solution:** $\lambda^2 - 4\lambda + 3 = (\lambda - 3)(\lambda - 1)$ 

---

### Exercise 3

Find eigenvector for $A = \begin{bmatrix} 2 & 0 \\ 0 & 3 \end{bmatrix}$ with $\lambda = 2$.

**Hint:** Solve $(A - 2I)\mathbf{v} = \mathbf{0}$.

**Solution:** $\mathbf{v} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$ (or any scalar multiple) 

---

### Exercise 4

If $A$ has eigenvalues 2, 3, 5, find $\det(A)$.

**Hint:** Use product formula.

**Solution:** $\det(A) = 2 \times 3 \times 5 = 30$ 

---

### Exercise 5

Can a $3 \times 3$ matrix with eigenvalues $1, 2, 3$ be diagonalized?

**Hint:** Distinct eigenvalues imply diagonalizability.

**Solution:** Yes, three distinct eigenvalues guarantee diagonalizability (Theorem 7.5). 

---

### Exercise 6

If $\lambda = 4$ is an eigenvalue of $A$, what is the corresponding eigenvalue of $A^2$?

**Hint:** Use property from Theorem 7.2.

**Solution:** $4^2 = 16$ 

---

### Exercise 7

Show that $A = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}$ has only one eigenvalue. Is it diagonalizable?

**Hint:** Compute characteristic polynomial; check for two independent eigenvectors.

**Solution:** Only $\lambda = 1$ (repeated); only one independent eigenvector, so **not** diagonalizable. 

## Applications

### Principal Component Analysis (PCA)

Eigenvalues and eigenvectors of covariance matrix identify principal directions of variance in data.

### Quantum Mechanics

Observables are represented by matrices; eigenvalues are measurable values, eigenvectors are quantum states.

### Google PageRank

Web page importance is the dominant eigenvector of the link matrix (eigenvalue 1).

### Differential Equations

Solutions to $\frac{d\mathbf{x}}{dt} = A\mathbf{x}$ involve eigenvectors and eigenvalues of $A$.

### Vibration Analysis

Natural frequencies of mechanical systems are eigenvalues; mode shapes are eigenvectors.

### Image Compression

Singular Value Decomposition (related to eigenvalues) compresses images efficiently.

## Common Mistakes

1. **Including zero vector:** Eigenvectors must be nonzero by definition.

2. **Forgetting $\lambda I$:** Characteristic equation is $\det(A - \lambda I) = 0$, not $\det(A) = \lambda$.

3. **Sign errors in characteristic polynomial:** Carefully expand determinant.

4. **Assuming diagonalizability:** Not all matrices are diagonalizable (e.g., defective matrices).

5. **Wrong eigenvector normalization:** Any nonzero scalar multiple is also an eigenvector.

## Summary

1. **Eigenvector:** $A\mathbf{v} = \lambda\mathbf{v}$ with $\mathbf{v} \neq \mathbf{0}$

2. **Eigenvalue:** Scalar $\lambda$ satisfying $\det(A - \lambda I) = 0$

3. **Finding eigenvalues:** Solve characteristic equation

4. **Finding eigenvectors:** Solve $(A - \lambda I)\mathbf{v} = \mathbf{0}$

5. **Diagonalization:** $A = PDP^{-1}$ where $P$ has eigenvectors as columns, $D$ is diagonal with eigenvalues

6. **Key properties:**
   - Distinct eigenvalues $\Rightarrow$ independent eigenvectors
   - $n$ distinct eigenvalues $\Rightarrow$ diagonalizable
   - $\det(A) = \prod \lambda_i$, trace$(A) = \sum \lambda_i$

---

**Further Study:** Jordan canonical form, generalized eigenvectors, spectral theorem, singular value decomposition.
