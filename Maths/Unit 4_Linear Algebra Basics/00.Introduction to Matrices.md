# Introduction to Matrices

## Learning Objectives

- Define matrices rigorously and understand matrix notation
- Master matrix operations: addition, scalar multiplication, multiplication
- Understand and work with special matrix types
- Prove fundamental properties of matrix operations
- Apply matrix operations to solve computational problems
- Recognize matrices in real-world applications

## Prerequisites

- Basic algebra and arithmetic
- Understanding of summation notation
- Familiarity with functions and mappings
- Elementary set theory

## Definition and Notation

### Definition 1.1 (Matrix)

An **$m \times n$ matrix** over a field $\mathbb{F}$ (typically $\mathbb{R}$ or $\mathbb{C}$) is a rectangular array of elements arranged in $m$ rows and $n$ columns:

$$A = \begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{bmatrix}$$

where $a_{ij} \in \mathbb{F}$ denotes the element in the $i$-th row and $j$-th column.

**Notation:** $A = [a_{ij}]_{m \times n}$ or simply $A = [a_{ij}]$ when dimensions are clear.

**Size:** The matrix has **order** or **dimension** $m \times n$ (read "$m$ by $n$").

### Definition 1.2 (Matrix Equality)

Two matrices $A = [a_{ij}]_{m \times n}$ and $B = [b_{ij}]_{p \times q}$ are **equal** if and only if:
1. $m = p$ and $n = q$ (same dimensions)
2. $a_{ij} = b_{ij}$ for all $1 \leq i \leq m, 1 \leq j \leq n$ (corresponding entries equal)

## Special Types of Matrices

### Definition 1.3 (Square Matrix)

A matrix $A$ is **square** of order $n$ if it has $n$ rows and $n$ columns ($m = n$).

### Definition 1.4 (Row and Column Matrices)

- A **row matrix** (or row vector) has dimension $1 \times n$: $R = [r_1\ r_2\ \cdots\ r_n]$
- A **column matrix** (or column vector) has dimension $m \times 1$: $C = \begin{bmatrix} c_1 \\ c_2 \\ \vdots \\ c_m \end{bmatrix}$

### Definition 1.5 (Zero Matrix)

The **zero matrix** $O_{m \times n}$ (or simply $O$) has all entries equal to zero: $[o_{ij}]$ where $o_{ij} = 0$ for all $i, j$.

### Definition 1.6 (Identity Matrix)

The **identity matrix** of order $n$, denoted $I_n$ or simply $I$, is the square matrix:

$$I_n = \begin{bmatrix}
1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1
\end{bmatrix}$$

Formally, $I_n = [\delta_{ij}]$ where $\delta_{ij}$ is the Kronecker delta:

$$\delta_{ij} = \begin{cases} 1 & \text{if } i = j \\ 0 & \text{if } i \neq j \end{cases}$$

### Definition 1.7 (Diagonal Matrix)

A square matrix $D = [d_{ij}]_{n \times n}$ is **diagonal** if $d_{ij} = 0$ for all $i \neq j$.

**Notation:** $D = \text{diag}(d_1, d_2, \ldots, d_n)$ where $d_i = d_{ii}$ are the diagonal entries.

### Definition 1.8 (Transpose)

The **transpose** of an $m \times n$ matrix $A = [a_{ij}]$ is the $n \times m$ matrix $A^T = [a_{ji}]$ obtained by interchanging rows and columns.

Formally: $(A^T)_{ij} = a_{ji}$

### Definition 1.9 (Symmetric and Skew-Symmetric Matrices)

A square matrix $A$ is:
- **Symmetric** if $A^T = A$ (i.e., $a_{ij} = a_{ji}$ for all $i, j$)
- **Skew-symmetric** if $A^T = -A$ (i.e., $a_{ij} = -a_{ji}$ for all $i, j$)

**Note:** If $A$ is skew-symmetric, then $a_{ii} = 0$ for all $i$ (diagonal entries are zero).

## Matrix Operations

### Definition 1.10 (Matrix Addition)

If $A = [a_{ij}]_{m \times n}$ and $B = [b_{ij}]_{m \times n}$ have the same dimensions, their **sum** is:

$$A + B = [a_{ij} + b_{ij}]_{m \times n}$$

Addition is defined **only** for matrices of the same size.

### Definition 1.11 (Scalar Multiplication)

If $A = [a_{ij}]_{m \times n}$ and $c \in \mathbb{F}$, the **scalar multiple** is:

$$cA = [ca_{ij}]_{m \times n}$$

### Definition 1.12 (Matrix Multiplication)

If $A = [a_{ij}]_{m \times p}$ and $B = [b_{jk}]_{p \times n}$, their **product** is $C = AB = [c_{ik}]_{m \times n}$ where:

$$c_{ik} = \sum_{j=1}^{p} a_{ij} b_{jk}$$

**Requirement:** Number of columns in $A$ must equal number of rows in $B$.

**Interpretation:** The $(i,k)$ entry of $AB$ is the dot product of the $i$-th row of $A$ with the $k$-th column of $B$.

## Fundamental Properties

### Theorem 1.1 (Properties of Matrix Addition)

For matrices $A, B, C$ of the same size:

1. **Commutativity:** $A + B = B + A$
2. **Associativity:** $(A + B) + C = A + (B + C)$
3. **Identity:** $A + O = A$
4. **Inverse:** $A + (-A) = O$ where $-A = (-1)A$

**Proof:** All properties follow directly from corresponding properties of real number addition applied componentwise. $\square$

### Theorem 1.2 (Properties of Scalar Multiplication)

For matrices $A, B$ and scalars $c, d$:

1. $c(A + B) = cA + cB$
2. $(c + d)A = cA + dA$
3. $(cd)A = c(dA)$
4. $1A = A$

**Proof:** Each property follows from distributive and associative properties of field operations applied componentwise. $\square$

### Theorem 1.3 (Properties of Matrix Multiplication)

For matrices $A, B, C$ (where products are defined) and scalar $c$:

1. **Associativity:** $(AB)C = A(BC)$
2. **Left Distributivity:** $A(B + C) = AB + AC$
3. **Right Distributivity:** $(A + B)C = AC + BC$
4. **Scalar Compatibility:** $c(AB) = (cA)B = A(cB)$
5. **Identity:** $AI_n = I_m A = A$ (for $A$ of size $m \times n$)

**Proof of (1):** Let $A = [a_{ij}]_{m \times p}$, $B = [b_{jk}]_{p \times q}$, $C = [c_{k\ell}]_{q \times n}$.

For $(AB)C$:
$$((AB)C)_{i\ell} = \sum_{k=1}^{q} (AB)_{ik} c_{k\ell} = \sum_{k=1}^{q} \left(\sum_{j=1}^{p} a_{ij}b_{jk}\right) c_{k\ell}$$

$$= \sum_{k=1}^{q} \sum_{j=1}^{p} a_{ij}b_{jk}c_{k\ell} = \sum_{j=1}^{p} a_{ij} \sum_{k=1}^{q} b_{jk}c_{k\ell} = \sum_{j=1}^{p} a_{ij}(BC)_{j\ell} = (A(BC))_{i\ell}$$

Thus $(AB)C = A(BC)$. $\square$

### Theorem 1.4 (Properties of Transpose)

For matrices $A, B$ (where operations are defined) and scalar $c$:

1. $(A^T)^T = A$
2. $(A + B)^T = A^T + B^T$
3. $(cA)^T = cA^T$
4. $(AB)^T = B^T A^T$ (reverse order!)

**Proof of (4):** Let $A = [a_{ij}]_{m \times p}$ and $B = [b_{jk}]_{p \times n}$.

$$((AB)^T)_{ki} = (AB)_{ik} = \sum_{j=1}^{p} a_{ij}b_{jk}$$

$$(B^T A^T)_{ki} = \sum_{j=1}^{p} (B^T)_{kj}(A^T)_{ji} = \sum_{j=1}^{p} b_{jk} a_{ij}$$

Since multiplication of scalars is commutative, these are equal. $\square$

### Theorem 1.5 (Non-Commutativity of Matrix Multiplication)

In general, $AB \neq BA$ even when both products are defined.

**Proof by Counterexample:**

Let $A = \begin{bmatrix} 1 & 2 \\ 0 & 0 \end{bmatrix}$ and $B = \begin{bmatrix} 0 & 0 \\ 3 & 4 \end{bmatrix}$

$$AB = \begin{bmatrix} 6 & 8 \\ 0 & 0 \end{bmatrix}, \quad BA = \begin{bmatrix} 0 & 0 \\ 3 & 6 \end{bmatrix}$$

Since $AB \neq BA$, matrix multiplication is not commutative. $\square$

## Worked Examples

### Example 1 (Simple): Matrix Addition and Scalar Multiplication

**Problem:** Given $A = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}$ and $B = \begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix}$, compute $2A - B$.

**Solution:**

First compute $2A$:
$$2A = 2\begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} = \begin{bmatrix} 2 & 4 \\ 6 & 8 \end{bmatrix}$$

Then compute $2A - B$:
$$2A - B = \begin{bmatrix} 2 & 4 \\ 6 & 8 \end{bmatrix} - \begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix} = \begin{bmatrix} 2-5 & 4-6 \\ 6-7 & 8-8 \end{bmatrix} = \begin{bmatrix} -3 & -2 \\ -1 & 0 \end{bmatrix}$$

**Answer:** $2A - B = \begin{bmatrix} -3 & -2 \\ -1 & 0 \end{bmatrix}$ $\square$

---

### Example 2 (Intermediate): Matrix Multiplication

**Problem:** Compute $AB$ where $A = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix}$ and $B = \begin{bmatrix} 1 & 4 \\ 2 & 5 \\ 3 & 6 \end{bmatrix}$.

**Solution:**

$A$ is $2 \times 3$ and $B$ is $3 \times 2$, so $AB$ is $2 \times 2$.

$$(AB)_{11} = (1)(1) + (2)(2) + (3)(3) = 1 + 4 + 9 = 14$$
$$(AB)_{12} = (1)(4) + (2)(5) + (3)(6) = 4 + 10 + 18 = 32$$
$$(AB)_{21} = (4)(1) + (5)(2) + (6)(3) = 4 + 10 + 18 = 32$$
$$(AB)_{22} = (4)(4) + (5)(5) + (6)(6) = 16 + 25 + 36 = 77$$

**Answer:** $AB = \begin{bmatrix} 14 & 32 \\ 32 & 77 \end{bmatrix}$ $\square$

---

### Example 3 (Intermediate): Transpose Properties

**Problem:** Given $A = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix}$ and $B = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$, verify that $(AB)^T = B^T A^T$.

**Solution:**

**Compute $AB$:**
$$AB = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix} = \begin{bmatrix} 1+4+9 \\ 4+10+18 \end{bmatrix} = \begin{bmatrix} 14 \\ 32 \end{bmatrix}$$

**Compute $(AB)^T$:**
$$(AB)^T = \begin{bmatrix} 14 & 32 \end{bmatrix}$$

**Compute $B^T A^T$:**
$$B^T = \begin{bmatrix} 1 & 2 & 3 \end{bmatrix}, \quad A^T = \begin{bmatrix} 1 & 4 \\ 2 & 5 \\ 3 & 6 \end{bmatrix}$$

$$B^T A^T = \begin{bmatrix} 1 & 2 & 3 \end{bmatrix} \begin{bmatrix} 1 & 4 \\ 2 & 5 \\ 3 & 6 \end{bmatrix} = \begin{bmatrix} 1+4+9 & 4+10+18 \end{bmatrix} = \begin{bmatrix} 14 & 32 \end{bmatrix}$$

**Verification:** $(AB)^T = B^T A^T$ ✓ $\square$

---

### Example 4 (Challenging): Symmetric and Skew-Symmetric Decomposition

**Problem:** Prove that any square matrix $A$ can be uniquely written as $A = S + K$ where $S$ is symmetric and $K$ is skew-symmetric.

**Solution:**

**Existence:** Define
$$S = \frac{1}{2}(A + A^T), \quad K = \frac{1}{2}(A - A^T)$$

**Check $S$ is symmetric:**
$$S^T = \left(\frac{1}{2}(A + A^T)\right)^T = \frac{1}{2}(A^T + (A^T)^T) = \frac{1}{2}(A^T + A) = S$$ ✓

**Check $K$ is skew-symmetric:**
$$K^T = \left(\frac{1}{2}(A - A^T)\right)^T = \frac{1}{2}(A^T - (A^T)^T) = \frac{1}{2}(A^T - A) = -K$$ ✓

**Check $S + K = A$:**
$$S + K = \frac{1}{2}(A + A^T) + \frac{1}{2}(A - A^T) = \frac{1}{2}(2A) = A$$ ✓

**Uniqueness:** Suppose $A = S_1 + K_1 = S_2 + K_2$ with $S_i$ symmetric and $K_i$ skew-symmetric.

Then $S_1 - S_2 = K_2 - K_1$. The left side is symmetric, the right side is skew-symmetric. A matrix that is both symmetric and skew-symmetric must be zero (proof: if $M^T = M$ and $M^T = -M$, then $M = -M$, so $2M = O$, thus $M = O$).

Therefore $S_1 = S_2$ and $K_1 = K_2$. $\square$

---

### Example 5 (Challenging): Matrix Powers and Pattern Recognition

**Problem:** Let $A = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}$. Compute $A^2, A^3, A^4$ and find a general formula for $A^n$.

**Solution:**

**Compute $A^2$:**
$$A^2 = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix} 0 & 0 \\ 0 & 0 \end{bmatrix} = O$$

**Compute $A^3$:**
$$A^3 = A^2 \cdot A = O \cdot A = O$$

**General formula:** Since $A^2 = O$, we have $A^n = O$ for all $n \geq 2$.

**Summary:**
$$A^n = \begin{cases} I & n = 0 \\ A & n = 1 \\ O & n \geq 2 \end{cases}$$

**Remark:** Such a matrix is called **nilpotent** (reaches zero under repeated multiplication). $\square$

---

### Example 6 (Application): Linear Transformation

**Problem:** A $2 \times 2$ matrix $A = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}$ represents rotation by angle $\theta$ counterclockwise. Verify that rotating by $\theta$ then by $\phi$ equals rotating by $\theta + \phi$.

**Solution:**

Let $R_\theta = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}$ and $R_\phi = \begin{bmatrix} \cos\phi & -\sin\phi \\ \sin\phi & \cos\phi \end{bmatrix}$

Compute $R_\phi R_\theta$:

$$(R_\phi R_\theta)_{11} = \cos\phi \cos\theta - \sin\phi \sin\theta = \cos(\theta + \phi)$$
$$(R_\phi R_\theta)_{12} = -\cos\phi \sin\theta - \sin\phi \cos\theta = -\sin(\theta + \phi)$$
$$(R_\phi R_\theta)_{21} = \sin\phi \cos\theta + \cos\phi \sin\theta = \sin(\theta + \phi)$$
$$(R_\phi R_\theta)_{22} = -\sin\phi \sin\theta + \cos\phi \cos\theta = \cos(\theta + \phi)$$

Therefore:
$$R_\phi R_\theta = \begin{bmatrix} \cos(\theta+\phi) & -\sin(\theta+\phi) \\ \sin(\theta+\phi) & \cos(\theta+\phi) \end{bmatrix} = R_{\theta+\phi}$$

This confirms the composition property of rotations. $\square$

## Exercises

### Exercise 1

Given $A = \begin{bmatrix} 2 & -1 \\ 0 & 3 \end{bmatrix}$ and $B = \begin{bmatrix} 1 & 4 \\ -2 & 5 \end{bmatrix}$, compute $3A + 2B$.

**Hint:** Apply scalar multiplication first, then add componentwise.

**Solution:**
$$3A = \begin{bmatrix} 6 & -3 \\ 0 & 9 \end{bmatrix}, \quad 2B = \begin{bmatrix} 2 & 8 \\ -4 & 10 \end{bmatrix}$$
$$3A + 2B = \begin{bmatrix} 8 & 5 \\ -4 & 19 \end{bmatrix}$$ ✓

---

### Exercise 2

Compute $AB$ and $BA$ for $A = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}$ and $B = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}$. Are they equal?

**Hint:** Carefully apply the definition of matrix multiplication for each product.

**Solution:**
$$AB = \begin{bmatrix} 2 & 1 \\ 4 & 3 \end{bmatrix}, \quad BA = \begin{bmatrix} 3 & 4 \\ 1 & 2 \end{bmatrix}$$
No, $AB \neq BA$. ✓

---

### Exercise 3

Find all $2 \times 2$ matrices $A$ such that $A^2 = I$.

**Hint:** Let $A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}$ and compute $A^2$, then solve the system of equations.

**Solution:** Solutions include $I, -I, \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}, \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}, \begin{bmatrix} -1 & 0 \\ 0 & 1 \end{bmatrix}$, and infinitely many others of the form $\begin{bmatrix} a & b \\ c & -a \end{bmatrix}$ where $a^2 + bc = 1$. ✓

---

### Exercise 4

Prove that if $A$ is symmetric and $B$ is symmetric, then $AB$ is symmetric if and only if $AB = BA$.

**Hint:** Use the transpose property $(AB)^T = B^T A^T$ and $A^T = A$, $B^T = B$.

**Solution:**
$(AB)^T = B^T A^T = BA$. So $AB$ is symmetric iff $(AB)^T = AB$ iff $BA = AB$. ✓

---

### Exercise 5

Show that for any matrix $A$, the matrix $AA^T$ is always symmetric.

**Hint:** Compute $(AA^T)^T$ and use transpose properties.

**Solution:**
$$(AA^T)^T = (A^T)^T A^T = AA^T$$
Thus $AA^T$ is symmetric. ✓

---

### Exercise 6

Let $A = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}$. Express $A$ as the sum of a symmetric matrix and a skew-symmetric matrix.

**Hint:** Use $S = \frac{1}{2}(A + A^T)$ and $K = \frac{1}{2}(A - A^T)$.

**Solution:**
$$A^T = \begin{bmatrix} 1 & 3 \\ 2 & 4 \end{bmatrix}$$
$$S = \frac{1}{2}\begin{bmatrix} 2 & 5 \\ 5 & 8 \end{bmatrix} = \begin{bmatrix} 1 & 5/2 \\ 5/2 & 4 \end{bmatrix}$$
$$K = \frac{1}{2}\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix} = \begin{bmatrix} 0 & -1/2 \\ 1/2 & 0 \end{bmatrix}$$
Verify: $S + K = A$ ✓

---

### Exercise 7

Prove that the product of two diagonal matrices is diagonal, and the diagonal entries of the product are the products of corresponding diagonal entries.

**Hint:** Let $D_1 = \text{diag}(d_1, \ldots, d_n)$ and $D_2 = \text{diag}(e_1, \ldots, e_n)$, then compute $(D_1 D_2)_{ij}$.

**Solution:** For $i \neq j$:
$$(D_1 D_2)_{ij} = \sum_{k=1}^n (D_1)_{ik}(D_2)_{kj} = (D_1)_{ii}(D_2)_{ij} = 0$$
For $i = j$:
$$(D_1 D_2)_{ii} = (D_1)_{ii}(D_2)_{ii} = d_i e_i$$
Thus $D_1 D_2 = \text{diag}(d_1 e_1, \ldots, d_n e_n)$. ✓

## Applications

### Computer Graphics

Matrices represent geometric transformations:
- **Translation, rotation, scaling** of objects
- **Projection** from 3D to 2D (perspective rendering)
- **Transformation pipelines** in graphics engines

### Data Science and Machine Learning

- **Data matrices:** Rows represent samples, columns represent features
- **Linear regression:** Solved using matrix operations $(X^T X)^{-1}X^T y$
- **Neural networks:** Weight matrices transform inputs through layers
- **Principal Component Analysis (PCA):** Dimensionality reduction via matrix decomposition

### Quantum Computing

- **Quantum gates:** Represented as unitary matrices
- **State vectors:** Column matrices representing quantum states
- **Evolution:** Matrix multiplication describes quantum state evolution

### Economics and Operations Research

- **Input-output models:** Leontief matrices model economic sectors
- **Network flow:** Adjacency matrices represent transportation networks
- **Optimization:** Linear programming uses matrix formulations

## Common Mistakes

1. **Dimension mismatch:** Cannot add matrices of different sizes; for multiplication, inner dimensions must match.

2. **Assuming commutativity:** In general, $AB \neq BA$. Always maintain order.

3. **Transpose of product:** $(AB)^T = B^T A^T$ (reverse order), not $A^T B^T$.

4. **Identity confusion:** $I_m A \neq A I_n$ unless $m = n$ (requires square matrix for both sides).

5. **Zero divisors:** $AB = O$ does not imply $A = O$ or $B = O$ (unlike scalar multiplication).

## Summary

1. **Matrix:** Rectangular array of numbers with dimension $m \times n$

2. **Special matrices:** Zero, identity, diagonal, symmetric, skew-symmetric

3. **Operations:**
   - Addition: $[a_{ij}] + [b_{ij}] = [a_{ij} + b_{ij}]$ (same size required)
   - Scalar multiplication: $c[a_{ij}] = [ca_{ij}]$
   - Multiplication: $(AB)_{ik} = \sum_j a_{ij}b_{jk}$ (inner dimensions must match)

4. **Properties:**
   - Addition is commutative and associative
   - Multiplication is associative but **not commutative**
   - Distributive laws hold
   - $(AB)^T = B^T A^T$

5. **Applications:** Computer graphics, machine learning, quantum computing, economics

---

**Further Study:** Matrix inverses, determinants, eigenvalues and eigenvectors, matrix decompositions (LU, QR, SVD).
