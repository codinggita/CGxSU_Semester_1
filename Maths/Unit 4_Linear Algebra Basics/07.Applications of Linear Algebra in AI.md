# Applications of Linear Algebra in Artificial Intelligence

## Learning Objectives

- Understand how linear algebra underpins modern AI and machine learning
- Apply matrix operations to neural networks and deep learning
- Recognize linear algebra in dimensionality reduction techniques
- Connect eigenvalues to principal component analysis
- Apply linear algebra concepts to computer vision and NLP
- Understand optimization in AI through linear algebra

## Prerequisites

- Matrix operations, rank, and inverses
- Eigenvalues and eigenvectors
- Vector operations and dot products
- Linear systems and transformations
- Basic understanding of AI/ML concepts

## Overview

Linear algebra is the mathematical foundation of artificial intelligence and machine learning. Every major AI techniquefrom neural networks to computer visionrelies fundamentally on linear algebraic operations.

**Core principle:** AI transforms high-dimensional data using linear and nonlinear operations built on matrices and vectors.

## Key Application Areas

### 1. Neural Networks and Deep Learning

**Foundation:** Matrix multiplication, vector operations

#### Forward Propagation

In a neural network layer:

$$\mathbf{a}^{[l]} = g(W^{[l]} \mathbf{a}^{[l-1]} + \mathbf{b}^{[l]})$$

where:
- $W^{[l]}$ is the weight matrix (learnable parameters)
- $\mathbf{a}^{[l-1]}$ is the activation vector from previous layer
- $\mathbf{b}^{[l]}$ is the bias vector
- $g$ is the activation function (nonlinearity)

**Matrix form for batch processing:**

$$A^{[l]} = g(W^{[l]} A^{[l-1]} + B^{[l]})$$

where $A^{[l]}$ is an $n^{[l]} \times m$ matrix ($m$ examples, $n^{[l]}$ neurons).

**Key insight:** Each layer performs affine transformation $W\mathbf{x} + \mathbf{b}$ followed by nonlinearity.

#### Backpropagation

Gradient computation uses:
- **Chain rule:** Multiplying Jacobian matrices
- **Matrix transposes:** $\frac{\partial L}{\partial W} = \frac{\partial L}{\partial Z} A^T$

**Example:** For a 3-layer network (input ’ hidden ’ output):

$$\mathbf{y} = W_3 \sigma(W_2 \sigma(W_1 \mathbf{x}))$$

where $W_1 \in \mathbb{R}^{h \times d}$, $W_2 \in \mathbb{R}^{k \times h}$, $W_3 \in \mathbb{R}^{c \times k}$.

### 2. Principal Component Analysis (PCA)

**Foundation:** Eigenvalues, eigenvectors, covariance matrices

#### Algorithm

Given data matrix $X \in \mathbb{R}^{n \times d}$ ($n$ samples, $d$ features):

1. **Center data:** $\tilde{X} = X - \mathbf{1}\boldsymbol{\mu}^T$ where $\boldsymbol{\mu} = \frac{1}{n}\sum \mathbf{x}_i$

2. **Compute covariance:** $C = \frac{1}{n}\tilde{X}^T \tilde{X} \in \mathbb{R}^{d \times d}$

3. **Find eigenvectors:** Solve $C\mathbf{v} = \lambda \mathbf{v}$

4. **Select top $k$ principal components:** Eigenvectors with largest eigenvalues

5. **Project data:** $Z = \tilde{X} V_k$ where $V_k$ contains top $k$ eigenvectors

**Dimensionality reduction:** $d$ dimensions ’ $k$ dimensions while preserving maximum variance.

**Variance explained:** $\frac{\sum_{i=1}^k \lambda_i}{\sum_{i=1}^d \lambda_i}$

### 3. Singular Value Decomposition (SVD)

**Foundation:** Matrix factorization, orthogonal matrices

#### Theorem (SVD)

Any $m \times n$ matrix $A$ can be factored as:

$$A = U\Sigma V^T$$

where:
- $U \in \mathbb{R}^{m \times m}$ is orthogonal (left singular vectors)
- $\Sigma \in \mathbb{R}^{m \times n}$ is diagonal with singular values $\sigma_1 \geq \sigma_2 \geq \cdots \geq 0$
- $V \in \mathbb{R}^{n \times n}$ is orthogonal (right singular vectors)

#### Applications in AI

**Low-rank approximation:**

$$A \approx A_k = U_k \Sigma_k V_k^T$$

using only top $k$ singular values.

**Image compression:** Store $k(m+n+1)$ values instead of $mn$.

**Recommender systems:** Matrix completion for collaborative filtering (Netflix prize).

**Latent Semantic Analysis (LSA):** Document-term matrices for NLP.

### 4. Linear Regression

**Foundation:** Solving linear systems, matrix inverses, projection

#### Ordinary Least Squares (OLS)

Minimize $\|X\boldsymbol{\beta} - \mathbf{y}\|^2$ where $X \in \mathbb{R}^{n \times d}$ is data, $\mathbf{y} \in \mathbb{R}^n$ is target.

**Normal equations:**

$$X^T X \boldsymbol{\beta} = X^T \mathbf{y}$$

**Solution (if $X^T X$ invertible):**

$$\boldsymbol{\beta} = (X^T X)^{-1} X^T \mathbf{y}$$

**Geometric interpretation:** $X\boldsymbol{\beta}$ is the orthogonal projection of $\mathbf{y}$ onto column space of $X$.

**Matrix form:** $\boldsymbol{\beta} = X^+ \mathbf{y}$ where $X^+ = (X^T X)^{-1}X^T$ is the **pseudoinverse**.

### 5. Support Vector Machines (SVM)

**Foundation:** Hyperplanes, dot products, optimization

#### Linear SVM

Find hyperplane $\mathbf{w}^T \mathbf{x} + b = 0$ that maximally separates classes.

**Optimization:**

$$\min_{\mathbf{w}, b} \frac{1}{2}\|\mathbf{w}\|^2$$

subject to $y_i(\mathbf{w}^T \mathbf{x}_i + b) \geq 1$ for all $i$.

**Decision function:** $f(\mathbf{x}) = \text{sign}(\mathbf{w}^T \mathbf{x} + b)$

**Kernel trick:** Use dot products $\mathbf{x}_i \cdot \mathbf{x}_j$ (can be replaced with kernel $K(\mathbf{x}_i, \mathbf{x}_j)$).

### 6. Word Embeddings (NLP)

**Foundation:** Vector spaces, cosine similarity

#### Word2Vec / GloVe

Represent words as vectors $\mathbf{w} \in \mathbb{R}^d$ (e.g., $d = 300$).

**Semantic relationships:**

$$\mathbf{w}_{\text{king}} - \mathbf{w}_{\text{man}} + \mathbf{w}_{\text{woman}} \approx \mathbf{w}_{\text{queen}}$$

**Similarity:** Cosine similarity

$$\text{sim}(\mathbf{u}, \mathbf{v}) = \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{u}\| \|\mathbf{v}\|} = \cos\theta$$

**Matrix factorization view:** Word co-occurrence matrix $M \approx WC^T$ where $W$ contains word vectors.

### 7. Convolutional Neural Networks (Computer Vision)

**Foundation:** Matrix convolution, filters as matrices

#### Convolution Operation

For image $I$ and filter/kernel $K$:

$$(I * K)_{ij} = \sum_{m,n} I_{i+m, j+n} K_{m,n}$$

**Matrix form:** Can be reformulated as matrix multiplication using Toeplitz matrices.

**Filters:** Small matrices (e.g., $3 \times 3$) that detect patterns (edges, textures).

**Pooling:** Downsampling operation (max/average over regions).

### 8. Attention Mechanisms and Transformers

**Foundation:** Matrix multiplication, softmax, linear transformations

#### Self-Attention

For sequence of vectors $X = [\mathbf{x}_1, \ldots, \mathbf{x}_n] \in \mathbb{R}^{n \times d}$:

1. **Compute queries, keys, values:**
   $$Q = XW_Q, \quad K = XW_K, \quad V = XW_V$$

2. **Attention scores:**
   $$A = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)$$

3. **Output:**
   $$\text{Attention}(Q, K, V) = AV$$

**All operations are matrix multiplications!**

**Multi-head attention:** Concatenate multiple attention outputs.

### 9. Gradient Descent Optimization

**Foundation:** Vectors, norms, matrix-vector products

#### Update Rule

$$\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta \nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta}_t)$$

where:
- $\boldsymbol{\theta} \in \mathbb{R}^d$ is parameter vector
- $\eta$ is learning rate (scalar)
- $\nabla L$ is gradient vector

**Momentum:**

$$\mathbf{v}_{t+1} = \beta \mathbf{v}_t + \nabla L(\boldsymbol{\theta}_t)$$
$$\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta \mathbf{v}_{t+1}$$

**Adam optimizer:** Uses first and second moments of gradients (vectors).

### 10. Graph Neural Networks

**Foundation:** Adjacency matrices, graph Laplacian

#### Message Passing

For graph with adjacency matrix $A \in \{0,1\}^{n \times n}$ and node features $H \in \mathbb{R}^{n \times d}$:

$$H^{(l+1)} = \sigma(\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2} H^{(l)} W^{(l)})$$

where:
- $\tilde{A} = A + I$ (adjacency with self-loops)
- $\tilde{D}$ is degree matrix
- $W^{(l)}$ is learnable weight matrix

**Matrix operations:** Normalize adjacency, multiply by features and weights.

## Worked Examples

### Example 1: Neural Network Forward Pass

**Problem:** Compute output for a 2-layer network with:
- Input: $\mathbf{x} = \begin{bmatrix} 1 \\ 2 \end{bmatrix}$
- Weights: $W_1 = \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 1 & 1 \end{bmatrix}$, $W_2 = \begin{bmatrix} 1 & 1 & 0 \end{bmatrix}$
- Activation: ReLU (ignore biases)

**Solution:**

Layer 1:
$$\mathbf{z}_1 = W_1 \mathbf{x} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 1 & 1 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \end{bmatrix} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$$

$$\mathbf{a}_1 = \text{ReLU}(\mathbf{z}_1) = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$$

Layer 2:
$$\mathbf{z}_2 = W_2 \mathbf{a}_1 = \begin{bmatrix} 1 & 1 & 0 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix} = 3$$

**Answer:** Output = 3 $\square$

---

### Example 2: PCA Dimensionality Reduction

**Problem:** Given centered data $X = \begin{bmatrix} 1 & 2 \\ -1 & -2 \\ 0 & 0 \end{bmatrix}$ (3 samples, 2 features), find the first principal component.

**Solution:**

Covariance matrix:
$$C = \frac{1}{3}X^T X = \frac{1}{3}\begin{bmatrix} 2 & 4 \\ 4 & 8 \end{bmatrix} = \begin{bmatrix} 2/3 & 4/3 \\ 4/3 & 8/3 \end{bmatrix}$$

Eigenvalues: $\det(C - \lambda I) = 0$
$$\lambda_1 = 10/3, \quad \lambda_2 = 0$$

First eigenvector (for $\lambda_1 = 10/3$):
$$\mathbf{v}_1 = \begin{bmatrix} 1 \\ 2 \end{bmatrix}$$ (unnormalized)

Normalized: $\hat{\mathbf{v}}_1 = \frac{1}{\sqrt{5}}\begin{bmatrix} 1 \\ 2 \end{bmatrix}$

**Answer:** First PC direction is $\begin{bmatrix} 1/\sqrt{5} \\ 2/\sqrt{5} \end{bmatrix}$ $\square$

---

### Example 3: Cosine Similarity

**Problem:** Compute cosine similarity between word vectors $\mathbf{u} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$ and $\mathbf{v} = \begin{bmatrix} 2 \\ 1 \\ 0 \end{bmatrix}$.

**Solution:**

$$\mathbf{u} \cdot \mathbf{v} = 2 + 2 + 0 = 4$$

$$\|\mathbf{u}\| = \sqrt{14}, \quad \|\mathbf{v}\| = \sqrt{5}$$

$$\text{sim}(\mathbf{u}, \mathbf{v}) = \frac{4}{\sqrt{14} \cdot \sqrt{5}} = \frac{4}{\sqrt{70}} \approx 0.478$$

**Answer:** Cosine similarity H 0.478 (moderately similar) $\square$

## Exercises

### Exercise 1

For a neural network layer with $W = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}$, $\mathbf{x} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}$, $\mathbf{b} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$, compute $W\mathbf{x} + \mathbf{b}$.

**Solution:** $\begin{bmatrix} 3 \\ 4 \end{bmatrix}$ 

---

### Exercise 2

If a covariance matrix has eigenvalues 9, 1, what fraction of variance is explained by the first PC?

**Solution:** $\frac{9}{9+1} = 0.9 = 90\%$ 

---

### Exercise 3

Verify that $A = U\Sigma V^T$ for $A = \begin{bmatrix} 3 & 0 \\ 0 & 2 \end{bmatrix}$ with $U = V = I$ and $\Sigma = A$.

**Solution:** $IAI^T = A$ 

---

### Exercise 4

For linear regression with $X = \begin{bmatrix} 1 & 1 \\ 1 & 2 \end{bmatrix}$ and $\mathbf{y} = \begin{bmatrix} 2 \\ 3 \end{bmatrix}$, compute $X^T X$.

**Solution:** $\begin{bmatrix} 2 & 3 \\ 3 & 5 \end{bmatrix}$ 

---

### Exercise 5

Two word vectors are orthogonal. What is their cosine similarity?

**Solution:** $\cos(90°) = 0$ (no similarity) 

## Summary

1. **Neural Networks:** Matrix multiplications $W\mathbf{x} + \mathbf{b}$ at each layer

2. **PCA:** Eigendecomposition of covariance matrix for dimensionality reduction

3. **SVD:** Matrix factorization for compression, recommender systems

4. **Linear Regression:** $(X^T X)^{-1}X^T \mathbf{y}$ solves least squares

5. **SVM:** Hyperplane defined by $\mathbf{w}^T \mathbf{x} + b = 0$

6. **Word Embeddings:** Vectors in high-dimensional space; cosine similarity

7. **CNNs:** Convolution as matrix operation; filters detect patterns

8. **Attention:** $\text{softmax}(QK^T/\sqrt{d_k})V$ - all matrix products

9. **Optimization:** Gradient descent updates parameter vectors

10. **Graph NNs:** Message passing via adjacency matrix operations

**Bottom line:** Linear algebra is the computational engine of modern AIfrom data representation to model training to inference.

---

**Further Study:** Tensor operations, automatic differentiation, sparse matrices, numerical linear algebra, GPU acceleration.
