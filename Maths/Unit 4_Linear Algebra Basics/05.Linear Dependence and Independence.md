# Linear Dependence and Independence

## Learning Objectives

- Define linear combinations, linear dependence, and linear independence rigorously
- Determine whether vectors are linearly independent or dependent
- Understand the relationship between linear independence and rank
- Apply linear independence concepts to span and basis
- Prove fundamental theorems about linear independence
- Recognize independence in applications

## Prerequisites

- Vector operations (addition, scalar multiplication)
- Matrix operations and row reduction
- Systems of linear equations
- Basic understanding of vector spaces

## Linear Combinations

### Definition 6.1 (Linear Combination)

A **linear combination** of vectors $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k$ is any vector of the form:

$$c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_k \mathbf{v}_k$$

where $c_1, c_2, \ldots, c_k \in \mathbb{R}$ are scalars (called **coefficients** or **weights**).

**Example:** $2\mathbf{v}_1 - 3\mathbf{v}_2 + 5\mathbf{v}_3$ is a linear combination.

### Definition 6.2 (Span)

The **span** of vectors $\mathbf{v}_1, \ldots, \mathbf{v}_k$ is the set of all linear combinations:

$$\text{span}\{\mathbf{v}_1, \ldots, \mathbf{v}_k\} = \{c_1 \mathbf{v}_1 + \cdots + c_k \mathbf{v}_k : c_i \in \mathbb{R}\}$$

**Interpretation:** All vectors that can be "built" from $\mathbf{v}_1, \ldots, \mathbf{v}_k$.

**Example:** span$\{\mathbf{i}, \mathbf{j}\} = \mathbb{R}^2$ (the entire 2D plane).

## Linear Dependence and Independence

### Definition 6.3 (Linear Dependence)

Vectors $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k$ are **linearly dependent** if there exist scalars $c_1, c_2, \ldots, c_k$ (not all zero) such that:

$$c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_k \mathbf{v}_k = \mathbf{0}$$

**Equivalently:** At least one vector can be expressed as a linear combination of the others.

### Definition 6.4 (Linear Independence)

Vectors $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k$ are **linearly independent** if they are not linearly dependent.

**Equivalently:** The only solution to

$$c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_k \mathbf{v}_k = \mathbf{0}$$

is $c_1 = c_2 = \cdots = c_k = 0$ (the **trivial solution**).

**Interpretation:** No vector can be written as a linear combination of the others; vectors are "independent" in direction.

### Theorem 6.1 (Dependence Implies Redundancy)

If $\mathbf{v}_1, \ldots, \mathbf{v}_k$ are linearly dependent, then at least one vector is a linear combination of the others.

**Proof:** Suppose $c_1 \mathbf{v}_1 + \cdots + c_k \mathbf{v}_k = \mathbf{0}$ with some $c_j \neq 0$. Then:

$$\mathbf{v}_j = -\frac{c_1}{c_j}\mathbf{v}_1 - \cdots - \frac{c_k}{c_j}\mathbf{v}_k$$

(omitting the $j$-th term). $\square$

### Theorem 6.2 (Zero Vector Implies Dependence)

Any set of vectors containing $\mathbf{0}$ is linearly dependent.

**Proof:** Choose $c_0 = 1$ for the zero vector and $c_i = 0$ for all others. Then $1 \cdot \mathbf{0} + 0 \cdot \mathbf{v}_1 + \cdots = \mathbf{0}$ with not all coefficients zero. $\square$

### Theorem 6.3 (Two Vectors)

Two vectors $\mathbf{u}, \mathbf{v}$ are linearly dependent if and only if one is a scalar multiple of the other.

**Proof:**
$(\Rightarrow)$ If dependent, $c_1 \mathbf{u} + c_2 \mathbf{v} = \mathbf{0}$ with some $c_i \neq 0$. If $c_1 \neq 0$, then $\mathbf{u} = -\frac{c_2}{c_1}\mathbf{v}$.

$(\Leftarrow)$ If $\mathbf{u} = k\mathbf{v}$, then $\mathbf{u} - k\mathbf{v} = \mathbf{0}$ (nontrivial). $\square$

## Testing for Linear Independence

### Method 1: Matrix Method

**To test if columns $\mathbf{v}_1, \ldots, \mathbf{v}_k \in \mathbb{R}^n$ are independent:**

1. Form matrix $A = [\mathbf{v}_1\ \mathbf{v}_2\ \cdots\ \mathbf{v}_k]$ (vectors as columns)
2. Row reduce to echelon form
3. **Independent** $\iff$ every column contains a pivot $\iff$ rank$(A) = k$
4. **Dependent** $\iff$ at least one free variable $\iff$ rank$(A) < k$

**Equivalently:** Solve the homogeneous system $A\mathbf{c} = \mathbf{0}$. Independent iff only trivial solution.

### Theorem 6.4 (Independence and Rank)

Vectors $\mathbf{v}_1, \ldots, \mathbf{v}_k$ are linearly independent if and only if rank$\,([\mathbf{v}_1\ \cdots\ \mathbf{v}_k]) = k$.

### Theorem 6.5 (Too Many Vectors)

If $k > n$ (more vectors than dimension), then any set of $k$ vectors in $\mathbb{R}^n$ is linearly dependent.

**Proof:** The matrix $A$ is $n \times k$ with $k > n$. rank$(A) \leq n < k$, so not all columns have pivots. $\square$

**Example:** Any 4 vectors in $\mathbb{R}^3$ are linearly dependent.

## Basis and Dimension

### Definition 6.5 (Basis)

A **basis** for $\mathbb{R}^n$ is a set of vectors $\{\mathbf{v}_1, \ldots, \mathbf{v}_n\}$ that:
1. Are linearly independent
2. Span $\mathbb{R}^n$

**Key property:** Every vector in $\mathbb{R}^n$ can be uniquely expressed as a linear combination of basis vectors.

### Definition 6.6 (Standard Basis)

The **standard basis** for $\mathbb{R}^n$ is $\{\mathbf{e}_1, \ldots, \mathbf{e}_n\}$ where $\mathbf{e}_i$ has 1 in position $i$ and 0 elsewhere.

For $\mathbb{R}^3$: $\left\{\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}\right\} = \{\mathbf{i}, \mathbf{j}, \mathbf{k}\}$

### Theorem 6.6 (Dimension Theorem)

All bases for $\mathbb{R}^n$ have exactly $n$ vectors.

**Definition:** The **dimension** of $\mathbb{R}^n$ is $n$.

## Worked Examples

### Example 1 (Simple): Testing Two Vectors

**Problem:** Are $\mathbf{v}_1 = \begin{bmatrix} 1 \\ 2 \end{bmatrix}$ and $\mathbf{v}_2 = \begin{bmatrix} 2 \\ 4 \end{bmatrix}$ linearly independent?

**Solution:**

$\mathbf{v}_2 = 2\mathbf{v}_1$ (scalar multiple), so linearly dependent.

**Verification:** $2\mathbf{v}_1 - \mathbf{v}_2 = \mathbf{0}$ (nontrivial combination).

**Answer:** Linearly dependent. $\square$

---

### Example 2 (Simple): Three Vectors in $\mathbb{R}^2$

**Problem:** Can three vectors in $\mathbb{R}^2$ be linearly independent?

**Solution:**

By Theorem 6.5, any $k > n$ vectors in $\mathbb{R}^n$ are dependent. Here $k = 3 > 2 = n$.

**Answer:** No, always dependent. $\square$

---

### Example 3 (Intermediate): Matrix Method

**Problem:** Test if $\mathbf{v}_1 = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$, $\mathbf{v}_2 = \begin{bmatrix} 0 \\ 1 \\ 2 \end{bmatrix}$, $\mathbf{v}_3 = \begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}$ are independent.

**Solution:**

Form matrix and reduce:
$$A = \begin{bmatrix} 1 & 0 & 1 \\ 2 & 1 & 0 \\ 3 & 2 & 1 \end{bmatrix}$$

$R_2 - 2R_1 \to R_2$, $R_3 - 3R_1 \to R_3$:
$$\begin{bmatrix} 1 & 0 & 1 \\ 0 & 1 & -2 \\ 0 & 2 & -2 \end{bmatrix}$$

$R_3 - 2R_2 \to R_3$:
$$\begin{bmatrix} 1 & 0 & 1 \\ 0 & 1 & -2 \\ 0 & 0 & 2 \end{bmatrix}$$

Three pivots, rank$(A) = 3$ = number of vectors.

**Answer:** Linearly independent. $\square$

---

### Example 4 (Intermediate): Homogeneous System

**Problem:** For what value of $k$ are $\mathbf{v}_1 = \begin{bmatrix} 1 \\ 2 \end{bmatrix}$, $\mathbf{v}_2 = \begin{bmatrix} 3 \\ k \end{bmatrix}$ linearly dependent?

**Solution:**

Dependent iff $c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 = \mathbf{0}$ has nontrivial solution.

$$\begin{bmatrix} 1 & 3 \\ 2 & k \end{bmatrix} \begin{bmatrix} c_1 \\ c_2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$$

Nontrivial solution exists iff $\det\begin{bmatrix} 1 & 3 \\ 2 & k \end{bmatrix} = 0$:

$$k - 6 = 0 \Rightarrow k = 6$$

**Answer:** $k = 6$ $\square$

---

### Example 5 (Challenging): Basis Verification

**Problem:** Verify that $\left\{\begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \\ 1 \end{bmatrix}\right\}$ is a basis for $\mathbb{R}^3$.

**Solution:**

Need to show: (1) linearly independent and (2) span $\mathbb{R}^3$.

**(1) Independence:**

$$A = \begin{bmatrix} 1 & 1 & 0 \\ 1 & 0 & 1 \\ 0 & 1 & 1 \end{bmatrix}$$

$$\det(A) = 1(0 - 1) - 1(1 - 0) + 0 = -1 - 1 = -2 \neq 0$$

Since $\det(A) \neq 0$, rank$(A) = 3$, so independent. 

**(2) Span:** Since we have 3 independent vectors in $\mathbb{R}^3$, they automatically span $\mathbb{R}^3$. 

**Answer:** Yes, forms a basis. $\square$

---

### Example 6 (Challenging): Linear Combination

**Problem:** Express $\mathbf{b} = \begin{bmatrix} 5 \\ 6 \\ 7 \end{bmatrix}$ as a linear combination of $\mathbf{v}_1 = \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}$, $\mathbf{v}_2 = \begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}$, $\mathbf{v}_3 = \begin{bmatrix} 0 \\ 1 \\ 1 \end{bmatrix}$ (from Example 5).

**Solution:**

Solve $c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + c_3 \mathbf{v}_3 = \mathbf{b}$:

$$\begin{bmatrix} 1 & 1 & 0 \\ 1 & 0 & 1 \\ 0 & 1 & 1 \end{bmatrix} \begin{bmatrix} c_1 \\ c_2 \\ c_3 \end{bmatrix} = \begin{bmatrix} 5 \\ 6 \\ 7 \end{bmatrix}$$

Row reducing $[A|\mathbf{b}]$ (details omitted):

$$c_1 = 2, \quad c_2 = 3, \quad c_3 = 4$$

**Verification:** $2\begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix} + 3\begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix} + 4\begin{bmatrix} 0 \\ 1 \\ 1 \end{bmatrix} = \begin{bmatrix} 5 \\ 6 \\ 7 \end{bmatrix}$ 

**Answer:** $\mathbf{b} = 2\mathbf{v}_1 + 3\mathbf{v}_2 + 4\mathbf{v}_3$ $\square$

## Exercises

### Exercise 1

Are $\begin{bmatrix} 1 \\ 0 \end{bmatrix}$ and $\begin{bmatrix} 0 \\ 1 \end{bmatrix}$ linearly independent?

**Hint:** Check if either is a scalar multiple of the other.

**Solution:** Yes, independent (standard basis vectors). 

---

### Exercise 2

Show that $\begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$, $\begin{bmatrix} 2 \\ 4 \\ 6 \end{bmatrix}$ are linearly dependent.

**Hint:** Second vector is a scalar multiple.

**Solution:** $\mathbf{v}_2 = 2\mathbf{v}_1$, dependent. 

---

### Exercise 3

Can 5 vectors in $\mathbb{R}^4$ be linearly independent?

**Hint:** Apply Theorem 6.5.

**Solution:** No, $k = 5 > 4 = n$, so dependent. 

---

### Exercise 4

Test independence: $\mathbf{v}_1 = \begin{bmatrix} 1 \\ 2 \end{bmatrix}$, $\mathbf{v}_2 = \begin{bmatrix} 3 \\ 4 \end{bmatrix}$.

**Hint:** Check if matrix $[\ \mathbf{v}_1\ \mathbf{v}_2\ ]$ has rank 2.

**Solution:** $\det\begin{bmatrix} 1 & 3 \\ 2 & 4 \end{bmatrix} = -2 \neq 0$, independent. 

---

### Exercise 5

If $\{\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3\}$ is independent, is $\{\mathbf{v}_1, \mathbf{v}_2\}$ independent?

**Hint:** Subset of independent set.

**Solution:** Yes, any subset of independent vectors is independent. 

---

### Exercise 6

Express $\begin{bmatrix} 7 \\ 3 \end{bmatrix}$ as a linear combination of $\begin{bmatrix} 1 \\ 0 \end{bmatrix}$ and $\begin{bmatrix} 0 \\ 1 \end{bmatrix}$.

**Hint:** Use standard basis.

**Solution:** $7\begin{bmatrix} 1 \\ 0 \end{bmatrix} + 3\begin{bmatrix} 0 \\ 1 \end{bmatrix}$ 

---

### Exercise 7

Find rank of $A = \begin{bmatrix} 1 & 2 & 3 \\ 2 & 4 & 6 \end{bmatrix}$ and determine if columns are independent.

**Hint:** Second row is multiple of first.

**Solution:** rank$(A) = 1 < 3$, columns dependent. 

## Applications

### Data Science

**Feature selection:** Remove linearly dependent features (collinearity) to improve model performance and interpretability.

### Control Theory

**Controllability:** System is controllable iff certain vectors are linearly independent.

### Coding Theory

**Error correction codes:** Codewords must be linearly independent for reliable decoding.

### Computer Graphics

**Coordinate systems:** Basis vectors define coordinate frames for transformations.

### Quantum Computing

**Quantum states:** Orthonormal basis states span the Hilbert space.

## Common Mistakes

1. **Confusing dependence with orthogonality:** Linear dependence is about linear combinations, not angles.

2. **Assuming independence from non-parallelism:** In higher dimensions, non-parallel doesn't imply independent.

3. **Forgetting zero vector:** Any set containing $\mathbf{0}$ is dependent.

4. **Too many vectors:** More than $n$ vectors in $\mathbb{R}^n$ are always dependent.

5. **Basis requires both:** Must be independent **and** span to be a basis.

## Summary

1. **Linear combination:** $c_1 \mathbf{v}_1 + \cdots + c_k \mathbf{v}_k$

2. **Linear dependence:** $\exists$ nontrivial $c_i$ such that $\sum c_i \mathbf{v}_i = \mathbf{0}$

3. **Linear independence:** Only trivial solution to $\sum c_i \mathbf{v}_i = \mathbf{0}$

4. **Testing:** Form matrix, row reduce, check rank = number of vectors

5. **Key theorems:**
   - $k > n$ vectors in $\mathbb{R}^n$ are dependent
   - Independent iff rank = $k$
   - Basis: independent + span

---

**Further Study:** Vector spaces, subspaces, orthogonal bases, Gram-Schmidt orthogonalization.
